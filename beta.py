# app.py
# Rehab Strength Dashboard (Workouts + Sleep + Recovery)
# Run: streamlit run app.py

import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
import seaborn as sns
from matplotlib.ticker import MultipleLocator
import matplotlib.dates as mdates
import io
from datetime import date
import statsmodels.api as sm
import scipy.stats as stats
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix, classification_report, roc_curve

st.set_page_config(page_title="Rehab Strength APP", layout="wide")
st.title("üèãÔ∏è‚Äç‚ôÇÔ∏è Rehab Strength APP", text_alignment="center")
st.caption("Workouts (Strong) ‚Ä¢ Sleep (Sheets) ‚Ä¢ Recovery (Sigmoid)")
app_version = "V2.3.1"
st.caption(f"App Version: {app_version} ‚Ä¢ Updated: {datetime.now():%Y-%m-%d %H:%M}")
st.markdown("---")

# -------------------------
# Sidebar
# -------------------------
st.sidebar.header("‚öôÔ∏è Settings")
cva_dt = st.sidebar.date_input("CVA split date", value=datetime(2025, 5, 14))
smooth_days = st.sidebar.slider("Smoothing window (days)", 3, 30, 15, 1)
show_dark = st.sidebar.toggle("üåô Dark mode", value=True)

if show_dark:
    st.markdown(
        """
        <style>
        html, body, [data-testid="stAppViewContainer"] { background-color:#0e1117 !important; color:#e5e7eb !important; }
        [data-testid="stHeader"] { background: rgba(0,0,0,0); }
        </style>
        """,
        unsafe_allow_html=True
    )
    import matplotlib as mpl
    mpl.rcParams.update({
        "figure.facecolor": "#0e1117",
        "axes.facecolor": "#0e1117",
        "savefig.facecolor": "#0e1117",
        "text.color": "#e5e7eb",
        "axes.labelcolor": "#e5e7eb",
        "xtick.color": "#e5e7eb",
        "ytick.color": "#e5e7eb",
        "axes.edgecolor": "#e5e7eb",
        "grid.color": "#2d3748"
    })
else:
    import matplotlib as mpl
    mpl.rcParams.update(mpl.rcParamsDefault)

# -------------------------
# Helpers
# -------------------------

def make_unique_columns(cols):
    """Fix duplicate column names by suffixing .1 .2 ... (Streamlit upload sometimes preserves dupes)."""
    seen = {}
    out = []
    for c in cols:
        c = str(c).strip()
        if c not in seen:
            seen[c] = 0
            out.append(c)
        else:
            seen[c] += 1
            out.append(f"{c}.{seen[c]}")
    return out
def pick_col(df, candidates):
    """Return the first column in candidates that exists in df, else None."""
    if df is None:
        return None
    for c in candidates:
        if c in df.columns:
            return c
    return None

def coerce_date(df, col="Date"):
    if col in df.columns:
        df[col] = pd.to_datetime(df[col], errors="coerce").dt.floor("D")
    return df

def epley_1rm(w, r):
    try:
        w = float(w); r = float(r)
        if np.isnan(w) or np.isnan(r):
            return np.nan
        return w * (1.0 + r/30.0)
    except Exception:
        return np.nan

def safe_numeric(s):
    return pd.to_numeric(s, errors="coerce")

def daily_ma(series, window_days):
    # series is indexed by datetime daily (or resampled)
    return series.rolling(window=window_days, min_periods=max(1, window_days//2)).mean()

def weekly_bucket(dt_series):
    dt = pd.to_datetime(dt_series, errors="coerce")  # coerce bad strings to NaT
    return dt.dt.to_period("W-MON").dt.start_time

def plot_line(dfx, x, y, title, ylabel, xlabel="Date", marker="o", markersize=4, color=None, 
              show_grid=True, despine=True, rotate_x=False, date_locator=None, 
              date_formatter=None, linewidth=1.5):
    fig, ax = plt.subplots(figsize=(10, 4))
    ax.plot(dfx[x], dfx[y], marker=marker, markersize=markersize, color=color, linewidth=linewidth)
    ax.set_title(title)
    ax.set_xlabel(xlabel)
    ax.set_ylabel(ylabel)
    
    if show_grid:
        ax.grid(axis="y", alpha=0.25)
        ax.set_axisbelow(True)
    
    if rotate_x:
        ax.tick_params(axis='x', rotation=45)
    
    if date_locator:
        ax.xaxis.set_major_locator(date_locator)
    if date_formatter:
        ax.xaxis.set_major_formatter(date_formatter)
    if despine:
        sns.despine(ax=ax)
    
    st.pyplot(fig)

def plot_two_axis(dfx, x, y1, y2, title, y1_label, y2_label):
    fig, ax1 = plt.subplots(figsize=(10, 4))
    ax1.plot(dfx[x], dfx[y1], marker="o")
    ax1.set_xlabel("Date")
    ax1.set_ylabel(y1_label)
    ax1.grid(True, alpha=0.25)

    ax2 = ax1.twinx()
    ax2.plot(dfx[x], dfx[y2], marker="o", linestyle="--")
    ax2.set_ylabel(y2_label)

    ax1.set_title(title)
    st.pyplot(fig)

def week_bounds(today=None):
    """Monday -> Sunday"""
    if today is None:
        today = pd.Timestamp.today().normalize()
    else:
        today = pd.to_datetime(today).normalize()
    start = today - pd.Timedelta(days=today.weekday())
    end = start + pd.Timedelta(days=6)
    return start, end

def safe_minimal_last(df, date_col, value_col):
    if df is None or value_col is None:
        return None
    if date_col not in df.columns or value_col not in df.columns:
        return None
    tmp = df[[date_col, value_col]].copy()
    tmp = tmp.dropna(subset=[date_col, value_col]).sort_values(date_col)
    if tmp.empty:
        return None
    return tmp[value_col].iloc[-1]
def recovery_zone(x):
    if x is None or pd.isna(x):
        return "No data"
    if x >= 0.7: return "üü¢ ‚¨ÜÔ∏è Ready"
    if x >= 0.55: return "üü° Moderate"
    return "üî¥ ‚¨áÔ∏è Low"

def normality_test(series):
    """Perform Shapiro-Wilk test for normality."""
    series = series.dropna()
    stat, p_value = stats.shapiro(series)
    if p_value > 0.05:
        interpretation = "Data is normally distributed (fail to reject H0)"
    else:
        interpretation = "Data is not normally distributed (reject H0)"
    return p_value, interpretation

def outlier_dectection_iqr(series):
    """Detect outliers using the IQR method."""
    series = series.dropna()
    Q1 = series.quantile(0.25)
    Q3 = series.quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    outliers = series[(series < lower_bound) | (series > upper_bound)]
    return outliers

def outlier_detection_zscore_modified(series, threshold=3):
    """Detect outliers using Z-score method."""
    series = series.dropna()
    mad = np.abs(series - series.median()).median()
    modified_z_scores = 0.6745 * (series - series.median()) / mad
    outliers = series[np.abs(modified_z_scores) > threshold]
    return outliers

def correlation_insight(df, col1, col2):
    """Provide insight on correlation between two columns."""
    if df is None or col1 not in df.columns or col2 not in df.columns:
        return "Insufficient data for correlation analysis."
    corr_coef = df[[col1, col2]].dropna().corr().iloc[0, 1]
    if corr_coef == 1:
        return st.success(f"Perfect positive correlation (1.00) between {col1} and {col2}.")
    elif corr_coef > 0.7:
        return st.success(f"Strong positive correlation ({corr_coef:.2f}) between {col1} and {col2}.")
    elif corr_coef > 0.49:
        return st.info(f"Moderate positive correlation ({corr_coef:.2f}) between {col1} and {col2}.")
    elif corr_coef > 0:
        return st.warning(f"Weak or no significant correlation ({corr_coef:.2f}) between {col1} and {col2}.")
    elif corr_coef == -1:
        return st.success(f"Perfect negative correlation (1.00) between {col1} and {col2}.")
    elif corr_coef < -0.7:
        return st.success(f"Strong negative correlation ({corr_coef:.2f}) between {col1} and {col2}.")
    elif corr_coef < -0.49:
        return st.info(f"Moderate negative correlation ({corr_coef:.2f}) between {col1} and {col2}.")
    else:
        return st.warning(f"Weak or no significant correlation ({corr_coef:.2f}) between {col1} and {col2}.")
    
def fit_distribution(data): #Function to fit distributions and calculate AIC/BIC
    data = np.array(data)
    distribution = {
        "normal": stats.norm,
        "student-t": stats.t,
        "cauchy": stats.cauchy,
        "laplace": stats.laplace,
        "skewnorm": stats.skewnorm,
        # Recommended additions for positive skewed data
        "lognormal": stats.lognorm,      # Very common for flight hours
        "gamma": stats.gamma,            # Positive values, flexible shape
        "exponential": stats.expon,      # Simple right-skewed
        "weibull": stats.weibull_min,    # Reliability/lifetime data
    }
    results = []

    for name, dist in distribution.items():
        try:
            params = dist.fit(data)
            ll = np.sum(dist.logpdf(data, *params))
            k = len(params)
            aic = 2*k - 2*ll
            bic = np.log(len(data))*k - 2*ll
        
            results.append({
                "distribution": name,
                "params": params,
                "AIC": aic,
            "BIC": bic
            })
        except:
            # Handle exceptions for distributions that fail to fit
            pass
    return pd.DataFrame(results).sort_values("AIC")

def sleep_classifier(q):
    return 1 if q in ["Good", "Excellent"] else 0

def compute_ecdf(data, x, complementary=True):
    counter = np.sum(data > x) if complementary else np.sum(data <= x)
    return np.round(counter / len(data), 4)

def metrics_learning_curve(df, sample_size, predictors, H=30, min_train=10):
    df_sample = df.iloc[:sample_size].copy()

    # Require enough rows to form a stable train/test
    if len(df_sample) < (H + min_train):
        return None  # skip this checkpoint

    train = df_sample.iloc[:-H].copy()
    test  = df_sample.iloc[-H:].copy()

    X = sm.add_constant(train[predictors], has_constant="add")
    y = train["Score"]
    model = sm.OLS(y, X).fit(cov_type="HC3")

    X_test = sm.add_constant(test[predictors], has_constant="add")
    y_test = test["Score"]
    y_pred = model.predict(X_test)

    r2_train = model.rsquared
    r2_test = r2_score(y_test, y_pred)
    mse_train = mean_squared_error(y, model.fittedvalues)
    mse_test = mean_squared_error(y_test, y_pred)
    mae_train = mean_absolute_error(y, model.fittedvalues)
    mae_test = mean_absolute_error(y_test, y_pred)
    rmse_train = np.sqrt(mse_train)
    rmse_test = np.sqrt(mse_test)

    return {
        "Model_samples": sample_size,
        "Train size": len(train),
        "Test size": len(test),
        "Train MAE": mae_train,
        "Test MAE": mae_test,
        "Train RMSE": rmse_train,
        "Test RMSE": rmse_test,
        "Train MSE": mse_train,
        "Test MSE": mse_test,
        "Train R¬≤": r2_train,
        "Test R¬≤": r2_test
    }

def string_to_decimal_hours(time_str):
    if pd.isna(time_str):
        return np.nan
    time_str = time_str.strip()
    if 'h' in time_str and 'min' in time_str:
        hours, minutes = time_str.split('h')
        minutes = minutes.replace('min', '').strip()
        return float(hours.strip()) + float(minutes) / 60
    elif 'h' in time_str:
        hours = time_str.replace('h', '').strip()
        return float(hours)
    elif 'min' in time_str:
        minutes = time_str.replace('min', '').strip()
        return float(minutes) / 60
    else:
        return np.nan
    
# -------------------------
# Uploads (hidden after loaded)
# -------------------------

# ---------- helpers ----------
def all_loaded() -> bool:
    return all(st.session_state.get(k) is not None for k in ["df_workouts", "df_sleep", "df_recovery"])

def load_df_from_upload(uploaded_file):
    data = uploaded_file.getvalue()
    return pd.read_csv(io.BytesIO(data))

def normalize_workouts(df):
    if "DATE" in df.columns:
        df["DATE"] = pd.to_datetime(df["DATE"], errors="coerce")
    if "EXERCISE_NAME" in df.columns:
        df["EXERCISE_NAME"] = df["EXERCISE_NAME"].astype(str).str.strip()
    for col in ["WEIGHT_LBS", "REPS", "RPE", "VOLUME"]:
        if col in df.columns:
            df[col] = safe_numeric(df[col])

    if set(["WEIGHT_LBS", "REPS"]).issubset(df.columns):
        df["est_1RM"] = df.apply(lambda r: epley_1rm(r["WEIGHT_LBS"], r["REPS"]), axis=1)

    if "DATE" in df.columns:
        df["Date"] = df["DATE"].dt.floor("D")
        df["DAY"] = df["Date"]
    return df

def normalize_sleep(df):
    df.columns = make_unique_columns(df.columns)
    if "Date" not in df.columns:
        for cand in ["DATE", "day", "date"]:
            if cand in df.columns:
                df = df.rename(columns={cand: "Date"})
                break
    df = coerce_date(df, "Date")
    for cand in ["Score", "Wake Count", "Efficiency", "Asleep hrs", "InBed hrs",
                 "REM hrs", "Light hrs", "Deep hrs"]:
        if cand in df.columns:
            df[cand] = safe_numeric(df[cand])
    return df

def normalize_recovery(df):
    df.columns = make_unique_columns(df.columns)
    if "Date" not in df.columns:
        for cand in ["DATE", "day", "date"]:
            if cand in df.columns:
                df = df.rename(columns={cand: "Date"})
                break
    df = coerce_date(df, "Date")
    for cand in ["Sigmoid Recovery Score", "RECOVERY_SCORE_RAW", "Stress_prev_day",
                 "Overnight HRV", "Resting Heart Rate", "Score"]:
        if cand in df.columns:
            df[cand] = safe_numeric(df[cand])
    return df

# ---------- init state ----------
if "show_uploads" not in st.session_state:
    st.session_state.show_uploads = True

st.subheader("üì• Upload your cleaned CSVs")

# status + button
left, right = st.columns([3, 1])
with left:
    w_ok = "‚úÖ" if st.session_state.get("df_workouts") is not None else "‚¨úÔ∏è"
    s_ok = "‚úÖ" if st.session_state.get("df_sleep") is not None else "‚¨úÔ∏è"
    r_ok = "‚úÖ" if st.session_state.get("df_recovery") is not None else "‚¨úÔ∏è"
    st.caption(f"{w_ok} Workouts ‚Ä¢ {s_ok} Sleep ‚Ä¢ {r_ok} Recovery")

with right:
    if all_loaded():
        st.session_state.show_uploads = False
        if st.button("Change files"):
            for k in ["workouts", "sleep", "recovery", "df_workouts", "df_sleep", "df_recovery"]:
                st.session_state.pop(k, None)
            st.session_state.show_uploads = True
            st.rerun()
    else:
        st.session_state.show_uploads = True

# uploaders (fully hidden once loaded)
if st.session_state.show_uploads:
    with st.expander("Upload panel", expanded=True):
        c1, c2, c3 = st.columns(3)
        with c1:
            up_workouts = st.file_uploader("Workouts: clean_strong_workouts.csv", type=["csv"], key="workouts")
        with c2:
            up_sleep = st.file_uploader("Sleep: clean_sleep_data.csv", type=["csv"], key="sleep")
        with c3:
            up_recovery = st.file_uploader("Recovery: clean_recovery_data.csv", type=["csv"], key="recovery")

        # IMPORTANT: persist + normalize immediately when uploads exist
        if up_workouts is not None and st.session_state.get("df_workouts") is None:
            st.session_state.df_workouts = normalize_workouts(load_df_from_upload(up_workouts))

        if up_sleep is not None and st.session_state.get("df_sleep") is None:
            st.session_state.df_sleep = normalize_sleep(load_df_from_upload(up_sleep))

        if up_recovery is not None and st.session_state.get("df_recovery") is None:
            st.session_state.df_recovery = normalize_recovery(load_df_from_upload(up_recovery))


# Stop until all 3 are loaded (AFTER the persist step above)
# ---------- downstream dataframes (always from session_state) ---------
if not all_loaded():
    st.info("Upload the 3 CSVs to unlock the dashboard.")
    st.stop()
# Use dataframes ONLY from session_state from here onward
workouts = st.session_state.df_workouts
sleep    = st.session_state.df_sleep
recovery = st.session_state.df_recovery

# -------------------------
# Tabs
# -------------------------
tab0, tab1, tab2, tab3, tab4, tab5, tab6 = st.tabs(["üè† Home", "üèãÔ∏è Workouts", "üò¥ Sleep", "üß† Recovery", "üîó Correlations", "üìâ Stats", "‚öôÔ∏è Models"])
# =========================
# TAB 0 ‚Äî HOME
# =========================

with tab0:
    st.header("üè† Weekly Snapshot")
    start_wk, end_wk = week_bounds()
    st.caption(f"Week: {start_wk.date()} ‚Üí {end_wk.date()}")
    st.markdown("---")
    # compute last dates safely
    st.subheader("üîê Data Integrity")
    last_workouts = workouts["Date"].max() if "Date" in workouts.columns else workouts["DATE"].max()
    last_sleep    = sleep["Date"].max() if sleep is not None and "Date" in sleep.columns else None
    last_recovery = recovery["Date"].max() if recovery is not None and "Date" in recovery.columns else None

    # freshness (days old)
    today = pd.Timestamp.today().normalize()
    def age_days(ts):
        if ts is None or pd.isna(ts): 
            return None
        return int((today - pd.to_datetime(ts).normalize()).days)

    a_w = age_days(last_workouts)
    a_s = age_days(last_sleep)
    a_r = age_days(last_recovery)

    c1, c2, c3, c4 = st.columns([1.2, 1.2, 1.2, 2.4])
    c1.metric("Workouts last", f"{pd.to_datetime(last_workouts):%b %d}", f"{a_w}d old" if a_w is not None else "‚Äî",
              delta_arrow="off")
    c2.metric("Sleep last",    f"{pd.to_datetime(last_sleep):%b %d}" if last_sleep is not None else "‚Äî",
            f"{a_s}d old" if a_s is not None else "‚Äî", delta_arrow="off")
    c3.metric("Recovery last", f"{pd.to_datetime(last_recovery):%b %d}" if last_recovery is not None else "‚Äî",
            f"{a_r}d old" if a_r is not None else "‚Äî", delta_arrow="off")

    # optional: overall status label
    overall_age = max([x for x in [a_w, a_s, a_r] if x is not None], default=None)
    label = "üü¢ Fresh" if (overall_age is not None and overall_age <= 1) else ("üü° Slightly delayed" if (overall_age is not None and overall_age <= 3) else "üî¥ Outdated")
    c4.markdown(f"**Data status:** {label}")
    st.markdown("---")
     # -------------------------
    # Weekly workouts snapshot
    # -------------------------
    if workouts is None:
        st.info("Upload workouts CSV to see weekly snapshot.")
    else:
        if "DAY" not in workouts.columns or workouts["DAY"].isna().all():
            st.warning("Workouts CSV needs a DATE/Date column so the app can create 'DAY'.")
        else:
            wk = workouts[(workouts["DAY"] >= start_wk) & (workouts["DAY"] <= end_wk)].copy()

            # Workouts (sessions): unique (DAY, WORKOUT_NAME) if available
            if "WORKOUT_NAME" in wk.columns:
                workouts_count = wk.dropna(subset=["WORKOUT_NAME"]).groupby(["DAY", "WORKOUT_NAME"]).ngroups
            else:
                workouts_count = wk["DAY"].nunique()

            # Time exercised: MAX duration per session, then sum
            total_minutes = None
            if "DURATION_MIN" in wk.columns:
                wk["DURATION_MIN"] = pd.to_numeric(wk["DURATION_MIN"], errors="coerce")

                if "WORKOUT_NAME" in wk.columns:
                    total_minutes = (
                        wk.dropna(subset=["DURATION_MIN"])
                          .groupby(["DAY", "WORKOUT_NAME"])["DURATION_MIN"]
                          .max()
                          .sum()
                    )
                else:
                    total_minutes = wk.groupby("DAY")["DURATION_MIN"].max().sum()

            total_hours = None if total_minutes is None else float(total_minutes) / 60.0

            # -------------------------
            # Latest values (sleep/recovery)
            # -------------------------
            st.subheader("üìä Key Metrics")
            last_sigmoid = safe_minimal_last(recovery, "Date", "Sigmoid Recovery Score") if recovery is not None else None
            sleep_score_col = pick_col(recovery, ["Score", "Sleep Score", "SleepScore", "SCORE"])
            sleep_hrv_col   = pick_col(recovery, ["Overnight HRV", "Avg. HRV", "HRV", "7d Avg"])

            last_sleep_score = safe_minimal_last(recovery, "Date", sleep_score_col)
            last_hrv        = safe_minimal_last(recovery, "Date", sleep_hrv_col)
            c1, c2, c3, c4, c5 = st.columns(5)
            c1.metric("Workouts (week)", int(workouts_count) if workouts_count is not None else "‚Äî")
            c2.metric("Time exercised (hrs)", f"{total_hours:.1f}" if total_hours is not None else "‚Äî",
                      delta=f"4 Hrs goal", delta_arrow="off", 
                      delta_color="normal" if total_hours is not None and total_hours >=4 else "inverse")
            if last_sigmoid is None or pd.isna(last_sigmoid):
                c3.metric("Last Recovery", "‚Äî", "No data")
            else:
                c3.metric(
                    "Last Recovery",
                    f"{last_sigmoid:.3f}",
                    recovery_zone(last_sigmoid), delta_arrow="off",
                    delta_color="normal" if last_sigmoid is not None and last_sigmoid >= 0.7 else ("inverse" if last_sigmoid is not None and last_sigmoid >= 0.55 else "inverse"))            
            c4.metric("Last sleep score %", f"{float(last_sleep_score):.0f}" if last_sleep_score is not None else "‚Äî", f"Excellent" if last_sleep_score is not None and last_sleep_score >= 85 else ("Fair" if last_sleep_score is not None and last_sleep_score >= 70 else "Poor"), delta_arrow="off")
            c5.metric("Last HRV (ms)", f"{float(last_hrv):.0f}" if last_hrv is not None and str(last_hrv) != "nan" else "‚Äî",
                      delta="Bad" if last_hrv is not None and last_hrv < 45 else "Good" if last_hrv is not None and last_hrv <= 60 else "Excellent", 
                      delta_arrow="off", delta_color="normal" if last_hrv is not None and last_hrv >= 45 else ("inverse" if last_hrv is not None and last_hrv < 60 else "off"))

            st.subheader("üìà Recent Trends")
            last_sigmoid_nap = safe_minimal_last(recovery, "Date", "Sigmoid with Nap") if recovery is not None else None
            last_delta = safe_minimal_last(recovery, "Date", "DELTA_NAP") if recovery is not None else None
            last_nap_status = safe_minimal_last(recovery, "Date", "Nap_Status") if recovery is not None else None
            c1, c2, c3 = st.columns(3)
            c1.metric("Recovery with Nap", f"{last_sigmoid_nap:.3f}" if last_sigmoid_nap is not None else "‚Äî",
                      recovery_zone(last_sigmoid_nap), delta_arrow="off",
                      delta_color="normal" if last_sigmoid_nap is not None and last_sigmoid_nap > last_sigmoid else ("inverse" if last_sigmoid_nap is not None and last_sigmoid_nap < last_sigmoid else "off"))
            c2.metric("Œî Nap Effect", f"{last_delta:.3f}" if last_delta is not None else "‚Äî",
                      f"{'‚¨ÜÔ∏è Positive' if last_delta is not None and last_delta > 0 else ('‚¨áÔ∏è Negative' if last_delta is not None and last_delta < 0 else 'No effect')}",
                      delta_arrow="off", delta_color="normal" if last_delta is not None and last_delta > 0 else ("inverse" if last_delta is not None and last_delta < 0 else "off"))
            c3.metric("Last Nap Status", last_nap_status if last_nap_status is not None else "No nap", delta_arrow="off")
    st.markdown("---")

    #---------------------------
    # Naps summary
    #--------------------------
    col1, col2, col3 = st.columns(3)
    with col1:
        st.subheader("üí§ Naps logged")
        window = st.segmented_control(
            "Select nap days window for avg:",
            options=[7, 14, 30, 60, 90],
            default=14,
            key="nap_avg_window",
            selection_mode="single", format_func=lambda x: f"{x} days")
        if window is None:
            window = 14
            st.markdown("Please select a number of days")
        sleep = sleep.sort_values("Date", ascending=True)
        avg_nap = sleep.tail(window)["Asleep_Nap"].dropna().mean()
        try:
            if pd.isna(avg_nap):
                col1.metric(f"üí§ Avg nap time (last {window} days)", " 0 minutes")
                st.write("No Naps Logged")
            else:
                col1.metric(f"üí§ Avg nap time (last {window} days)", f"{avg_nap:.1f} minutes")
        except Exception as e:
            st.error(f"Error computing nap frequency: {e}")
            st.error("Please select a number")

    with col2:
        st.subheader("üìÖ Nap Days")
        window2 = st.segmented_control(
            "Select nap days window for avg:",
            options=[7, 14, 30, 60, 90],
            default=14,
            key="nap_avg_window2",
            selection_mode="single", format_func=lambda x: f"{x} days")
        try:
            if pd.isna(avg_nap):
                st.write("No Day Naps Logged")
            else:
                start_date = sleep["Date"].max() - pd.Timedelta(days=window2)
                nap_data = sleep[sleep["Date"] >= start_date].dropna(subset=["Asleep_Nap"])
                n_naps = nap_data.loc[nap_data["Asleep_Nap"].notna() & (nap_data["Asleep_Nap"] > 0)].shape[0]
                col2.metric("Naps logged", f"{n_naps}")
        except Exception as e:
            st.error(f"Error computing nap frequency: {e}")
            st.error("Please select a number")
    with col3:
        st.subheader("üìà Nap Frequency")
        window3 = st.segmented_control(
            "Select nap days window for avg:",
            options=[7, 14, 30, 60, 90],
            default=14,
            key="nap_avg_window3",
            selection_mode="single", format_func=lambda x: f"{x} days")
        try:
            if pd.isna(avg_nap):
                st.write("No Frequency Naps Logged")
            else:
                start_date = sleep["Date"].max() - pd.Timedelta(days=window3)
                nap_data = sleep[sleep["Date"] >= start_date].dropna(subset=["Asleep_Nap"])
                sleep_filtered = sleep[sleep["Date"] >= start_date]
                freq_val = (nap_data[nap_data["Asleep_Nap"].notna() & (nap_data["Asleep_Nap"] > 0)].shape[0] / sleep_filtered.shape[0]) * 100
                def freq_label(x):
                    if pd.isna(x):
                        return "No data"
                    if x <= 15: return "üî¥ Low"
                    if x <= 30: return "üü° Moderate"
                    return "üü¢ High"
                col3.metric("Nap frequency", f"{freq_val:.1f} %", delta=freq_label(freq_val))
                st.caption(f"Naps on {nap_data[nap_data['Asleep_Nap'].notna() & (nap_data['Asleep_Nap'] > 0)].shape[0]} of the last {window3} days")
        except Exception as e:
            st.error(f"Error computing nap frequency: {e}")
            st.error("Please select a number")
    st.markdown("---")
    # -------------------------
    # Quick trends (independent)
    # -------------------------
    left, right = st.columns(2)

    with left:
        st.subheader("üß† Recovery (last 14 days)")
        if recovery is not None and {"Date", "Sigmoid Recovery Score"}.issubset(recovery.columns):
            tmp = recovery.dropna(subset=["Date", "Sigmoid Recovery Score"]).sort_values("Date").tail(14)
            if tmp.empty:
                st.info("Recovery CSV loaded but no usable rows.")
            else:
                fig, ax = plt.subplots(figsize=(7,3))
                tmp_avg = tmp["Sigmoid Recovery Score"].mean()
                sld1 = st.slider("Select days for moving average", 2, 11, 5, 1, width=250)
                ax.plot(tmp["Date"], tmp["Sigmoid Recovery Score"], marker="o", markersize=3, color="green")
                roll_avg_recovery = tmp["Sigmoid Recovery Score"].rolling(window=sld1).mean()
                ax.plot(tmp["Date"], roll_avg_recovery, color="orange", linestyle="--", 
                        alpha=0.25, label=f"MA {sld1} days {roll_avg_recovery.iloc[-1]:.2f}")
                ax.axhline(tmp_avg, color="blue", linestyle=":", alpha=0.6, label=f"Avg {tmp_avg:.2f}")
                ax.set_xlabel("")
                ax.set_ylim(0, 1)
                ax.legend(loc="lower left")
                ax.tick_params(axis='x', rotation=45, labelsize=6)
                ax.xaxis.set_major_formatter(mdates.DateFormatter('%b-%d'))
                ax.xaxis.set_major_locator(mdates.DayLocator(interval=1))
                ax.set_ylabel("Sigmoid")
                sns.despine(ax=ax)
                st.pyplot(fig)
        else:
            st.info("No recovery data uploaded yet.")

    with right:
        st.subheader("üß† Recovery (last 14 days) with Nap")
        if recovery is not None and {"Date", "Sigmoid with Nap"}.issubset(recovery.columns):
            tmp2 = recovery.dropna(subset=["Date", "Sigmoid with Nap"]).sort_values("Date").tail(14)
            last_recovery_nap = tmp2["Sigmoid with Nap"].iloc[-1] if not tmp2.empty else None
            if tmp2.empty:
                st.info("Recovery CSV loaded but no usable rows.")
            else:
                fig, ax = plt.subplots(figsize=(7,3))
                tmp2_avg = tmp2["Sigmoid with Nap"].mean()
                sld_sig_nap = st.slider("Select days for moving average", 2, 11, 5, 1, width=250, key="sig_nap_ma_slider")
                ax.plot(tmp2["Date"], tmp2["Sigmoid with Nap"], marker="o", markersize=3, color="seagreen")
                roll_avg_recovery_nap = tmp2["Sigmoid with Nap"].rolling(window=sld_sig_nap).mean()
                ax.plot(tmp2["Date"], roll_avg_recovery_nap, color="orange", linestyle="--", 
                        alpha=0.25, label=f"MA {sld_sig_nap} days {roll_avg_recovery_nap.iloc[-1]:.2f}")
                ax.axhline(tmp2_avg, color="blue", linestyle=":", alpha=0.6, label=f"Avg {tmp2_avg:.2f}")
                ax.set_xlabel("")
                ax.set_ylim(0,1)
                ax.legend(loc="lower left")
                ax.tick_params(axis='x', rotation=45, labelsize=6)
                ax.xaxis.set_major_formatter(mdates.DateFormatter('%b-%d'))
                ax.xaxis.set_major_locator(mdates.DayLocator(interval=1))
                ax.set_ylabel("Sigmoid")
                sns.despine(ax=ax)
                st.pyplot(fig)
        else:
            st.info("No recovery data uploaded yet.")
    with st.expander("Recovery Insights", icon="üß†",expanded=False):
        vol_no_nap = recovery["Sigmoid Recovery Score"].tail(14).std()
        vol_nap = recovery["Sigmoid with Nap"].tail(14).std()
        delta_window = st.slider("Select days window for Œî average", 2, 14, 7, 1, key="delta_avg_window", width=200)
        recovery_naps = recovery.dropna(subset=["Date", "DELTA_NAP"]).sort_values("Date").tail(delta_window)
        avg_delta = recovery_naps["DELTA_NAP"].mean() if not recovery_naps.empty else None
        st.write(f"Average Œî {delta_window} days: {avg_delta:.2f}" if avg_delta is not None else "Average Œî not available")
        if last_sigmoid is not None and last_sigmoid >= tmp_avg:
            st.write(f"14-day Last Recovery without Nap: {last_sigmoid:.2f} above Avg")
        else:
            st.write(f"14-day Last Recovery without Nap: {last_sigmoid_nap:.2f} below Avg")
        if last_recovery_nap is not None and last_recovery_nap >=tmp2_avg:
            st.write(f"14-day Last Recovery with Nap: {last_recovery_nap:.2f} above Avg")
        else:
            st.write(f"14-day Last Recovery with Nap: {last_recovery_nap:.2f} below Avg")
        st.write(f"Volatility (STD) without Nap (14 days): {vol_no_nap:.4f}")
        st.write(f"Volatility (STD) with Nap (14 days): {vol_nap:.4f}")

    st.caption("Note: Recovery with Nap may be higher than without nap depending on nap effect.", help="Nap effect is computed based on the duration of the nap and the hour it was taken. A positive nap effect indicates that the nap contributed positively to recovery, while a negative effect suggests it may have disrupted sleep patterns.")
    st.markdown("---")
    st.subheader("üò¥ Sleep score (last 14 days)")
    sleep_score_col = pick_col(recovery, ["Score", "Sleep Score", "SleepScore", "SCORE", "Score.1", "Score.2"]) if recovery is not None else None

    if recovery is not None and sleep_score_col is not None and "Date" in recovery.columns:
        tmp = recovery.dropna(subset=["Date", sleep_score_col]).sort_values("Date").tail(14)
        if tmp.empty:
            st.info("Sleep CSV loaded but no usable rows.")
        else:
            fig, ax = plt.subplots(figsize=(7,3))
            sld2 = st.slider("Select days for moving average", 2, 11, 5, 1, key="sleep_ma_slider", width=250)
            roll_avg_sleep = tmp[sleep_score_col].rolling(window=sld2).mean()
            tmp_avg_sleep = tmp[sleep_score_col].mean()
            ax.axhline(tmp_avg_sleep, color="blue", linestyle=":", alpha=0.6, label=f"Avg {tmp_avg_sleep:.0f}")
            ax.plot(tmp["Date"], tmp[sleep_score_col], marker="o", markersize=3, color="purple")
            ax.plot(tmp["Date"],roll_avg_sleep, color="orange", linestyle="--", 
                    alpha=0.4, label=f"MA {sld2} days {roll_avg_sleep.iloc[-1]:.0f}")
            ax.set_xlabel("")
            ax.legend(loc="lower left")
            ax.set_ylim(50,100)
            ax.set_ylabel(sleep_score_col)
            ax.tick_params(axis='x', rotation=45, labelsize=6)
            ax.xaxis.set_major_formatter(mdates.DateFormatter('%b-%d'))
            ax.xaxis.set_major_locator(mdates.DayLocator(interval=1))
            sns.despine(ax=ax)
            st.pyplot(fig)
    else:
        st.info("No sleep data uploaded yet (or score column not detected).")
    st.markdown("---")

    with st.expander("üõ† Debug"):
        st.write("Workouts cols:", list(workouts.columns))
        st.write("Sleep cols:", list(sleep.columns))
        st.write("Recovery cols:", list(recovery.columns))
    st.markdown("--")
# =========================
# TAB 1 ‚Äî WORKOUTS
# =========================
with tab1:
    st.header("üèãÔ∏è Workouts")

    if workouts is None:
        st.info("Upload your cleaned workouts CSV to see charts.")
    else:
        # basic checks
        req = {"Date", "EXERCISE_NAME"}
        if not req.issubset(workouts.columns):
            st.error(f"Workouts CSV must include at least: {req}")
        else:
            cva_ts = pd.to_datetime(cva_dt)

            # pick exercise
            ex_list = sorted(workouts["EXERCISE_NAME"].dropna().unique())
            chosen_ex = st.selectbox("Choose an exercise:", ex_list)

            w = workouts[workouts["EXERCISE_NAME"] == chosen_ex].copy()
            w = w.dropna(subset=["Date"]).sort_values("Date")

            # -------- 1) Pre vs Post (Estimated 1RM mean)
            st.subheader("üìä Pre vs Post (Estimated 1RM)")
            if "est_1RM" in w.columns:
                pre = w[w["Date"] < cva_ts]["est_1RM"].mean()
                post = w[w["Date"] >= cva_ts]["est_1RM"].mean()

                fig, ax = plt.subplots(figsize=(6, 4))
                vals = [pre, post]
                labs = ["Pre-CVA", "Post-CVA"]
                ax.bar(np.arange(2), vals, width=0.6, edgecolor="black")
                ax.set_xticks(np.arange(2)); ax.set_xticklabels(labs)
                ax.set_ylabel("Estimated 1RM (lb)")
                ax.set_title(chosen_ex, fontsize=14, fontweight="bold", pad=15)
                sns.despine(ax=ax)
                for i, v in enumerate(vals):
                    if not np.isnan(v):
                        ax.text(i, v + 2, f"{v:.1f}", ha="center", va="bottom")
                ax.grid(True, axis="y", alpha=0.25)
                ax.set_axisbelow(True)
                st.pyplot(fig)
            else:
                st.info("No est_1RM found. Make sure WEIGHT_LBS and REPS exist in the workouts file.")

            # -------- 2) Progress over time (daily + MA)
            st.subheader("‚è≥ Progress over time (Daily + Moving Avg)")
            if "est_1RM" in w.columns:
                # daily mean est_1RM
                daily = w.groupby("Date", as_index=False)["est_1RM"].mean().sort_values("Date")
                daily["MA"] = daily_ma(daily["est_1RM"], smooth_days)

                fig, ax = plt.subplots(figsize=(10, 4))
                ax.plot(daily["Date"], daily["est_1RM"], marker="s", label="Daily mean est.1RM", color="salmon", markersize=4)
                ax.plot(daily["Date"], daily["MA"], linestyle="--", label=f"{smooth_days}-day MA", color="yellow")
                ax.axvline(cva_ts, linestyle=":", linewidth=1)
                ax.set_title(f"{chosen_ex} ‚Äî Comparative Pre & Post CVA", fontsize=14, fontweight="bold", pad=15)
                ax.set_xlabel("Date"); ax.set_ylabel("lb")
                ax.grid(axis="y", alpha=0.25)
                ax.set_axisbelow(True)
                sns.despine(ax=ax)
                ax.legend()
                st.pyplot(fig)

            # -------- 3) Weekly Volume chart (per exercise + total)
            st.subheader("üì¶ Weekly Volume (Exercise and Total)")
            if "VOLUME" in workouts.columns:
                workouts["Week"] = weekly_bucket(workouts["Date"])
                w_ex_week = workouts[workouts["EXERCISE_NAME"] == chosen_ex].groupby("Week", as_index=False)["VOLUME"].sum()
                w_all_week = workouts.groupby("Week", as_index=False)["VOLUME"].sum()

                cA, cB = st.columns(2)
                with cA:
                    plot_line(w_ex_week.sort_values("Week"), "Week", "VOLUME",
                              f"Weekly Volume ‚Äî {chosen_ex}", "Total Volume (lb¬∑reps)", xlabel="Week")
                with cB:
                    plot_line(w_all_week.sort_values("Week"), "Week", "VOLUME",
                              "Weekly Volume ‚Äî ALL Exercises", "Total Volume (lb¬∑reps)", xlabel="Week")

            # -------- 4) RPE trend (daily mean)
            st.subheader("üî• RPE Trend (Daily)")
            if "RPE" in w.columns:
                rpe_daily = w.groupby("Date", as_index=False)["RPE"].mean().sort_values("Date")
                if rpe_daily["RPE"].notna().sum() == 0:
                    st.info("No RPE values recorded for this exercise yet.")
                else:
                    rpe_daily["MA"] = daily_ma(rpe_daily["RPE"], smooth_days)
                    fig, ax = plt.subplots(figsize=(10, 4))
                    ax.plot(rpe_daily["Date"], rpe_daily["RPE"], marker="o", label="Daily mean RPE", color="salmon", markersize=4)
                    ax.plot(rpe_daily["Date"], rpe_daily["MA"], linestyle="--", label=f"{smooth_days}-day MA", color="yellow")
                    ax.axvline(cva_ts, linestyle=":", linewidth=1)
                    ax.set_title(f"{chosen_ex} ‚Äî RPE Trend & Post CVA", fontsize=14, fontweight="bold", pad=15)
                    ax.set_xlabel("Date"); ax.set_ylabel("RPE")
                    ax.grid(axis="y", alpha=0.25)
                    ax.set_axisbelow(True)
                    ax.tick_params(axis='x', rotation=45)
                    sns.despine(ax=ax)
                    ax.legend()
                    st.pyplot(fig)

            # -------- Summary table (exercise)
            with st.expander("üìã Show raw sets for this exercise"):
                st.dataframe(w.sort_values("DATE" if "DATE" in w.columns else "Date"))

# =========================
# TAB 2 ‚Äî SLEEP
# =========================
with tab2:
    st.header("üò¥ Sleep")

    if sleep is None:
        st.info("Upload your clean sleep CSV to see charts.")
    else:
        if "Date" not in sleep.columns:
            st.error("Sleep CSV must include a Date column.")
        else:
            sleep = sleep.sort_values("Date")
            st.subheader("üìã Sleep table")
            st.dataframe(sleep)

            # Score
            if "Score" in sleep.columns:
                st.subheader("‚≠ê Sleep Score")
                plot_line(sleep.dropna(subset=["Score"]), "Date", "Score", "Sleep Score over time", "Score")

            # Stages
            stage_cols = [c for c in ["REM hrs", "Light hrs", "Deep hrs"] if c in sleep.columns]
            if stage_cols:
                st.subheader("üß± Sleep Stages (hrs)")
                df_s = sleep[["Date"] + stage_cols].dropna(subset=["Date"]).copy()

                fig, ax = plt.subplots(figsize=(10, 4))
                bottom = np.zeros(len(df_s))
                for col in stage_cols:
                    vals = df_s[col].fillna(0).to_numpy()
                    ax.bar(df_s["Date"], vals, bottom=bottom, width=0.8, label=col)
                    bottom += vals
                ax.set_title("Sleep stages (stacked hours)", fontsize=14, fontweight="bold", pad=15)
                ax.set_xlabel("")
                ax.set_ylabel("Hours")
                sns.despine(ax=ax)
                ax.grid(axis="y", alpha=0.25)
                ax.tick_params(axis='x', rotation=45, labelsize=6)
                ax.xaxis.set_major_locator(mdates.MonthLocator(interval=1))
                ax.xaxis.set_major_formatter(mdates.DateFormatter('%b-%Y'))
                ax.set_axisbelow(True)
                ax.legend()
                st.pyplot(fig)

            # Wake Count
            if "Wake Count" in sleep.columns:
                st.subheader("üåô Wake Count")
                plot_line(sleep.dropna(subset=["Wake Count"]), "Date", "Wake Count", 
                        "Wake Count over time", "Count", 
                        marker=None, color="purple", xlabel="",
                        rotate_x=True, date_locator=mdates.MonthLocator(interval=2), linewidth=0.7)
            # Naps
            left, right = st.columns(2)
            if "Asleep_Nap" in sleep.columns:
                with left:
                    st.subheader("üí§ Nap Asleep (min)")
                    slider_nap2 = st.slider("Select number of days to show for Nap Asleep plot", 60, 365, 365, 1, key="nap_asleep_days")
                    recent_date = sleep["Date"].max()
                    start_date = recent_date - pd.Timedelta(days=slider_nap2)
                    filtered_nap = sleep[(sleep["Date"] >= start_date) & (sleep["Date"] <= recent_date)].copy()
                    df_plot = filtered_nap.dropna(subset=["Asleep_Nap"])
                    roll_avg_nap = df_plot["Asleep_Nap"].rolling(window=7, min_periods=1).mean()
                    

                    fig, ax = plt.subplots(figsize=(10, 4))
                    ax.plot(df_plot["Date"], df_plot["Asleep_Nap"], color="teal", linewidth=1.5, label="Nap Asleep")
                    ax.plot(df_plot["Date"], roll_avg_nap, marker="o", markersize=2, color="salmon", label=f"7-day MA", 
                            linewidth=1, linestyle="--")
                    ax.set_title("Nap Asleep over time")
                    ax.set_ylabel("Minutes")
                    ax.tick_params(axis='x', rotation=45)
                    ax.xaxis.set_major_locator(mdates.MonthLocator(interval=1))
                    ax.xaxis.set_major_formatter(mdates.DateFormatter('%b-%Y'))
                    ax.grid(axis="y", alpha=0.25)
                    ax.set_axisbelow(True)
                    ax.legend()
                    sns.despine(ax=ax)
                    st.pyplot(fig)

                    # Monthly total naps 
                # TBD

            else:
                st.info("Column 'InBed_Nap' not found in sleep data.")
            with right:
                st.subheader("üõèÔ∏è Nap Asleep (min)")
                if "Asleep_Nap" in sleep.columns:
                    slider_nap = st.slider("Select number of days to show for Nap Asleep plot", 60, 365, 365, 1, key="nap_asleep")
                    recent_date = sleep["Date"].max()
                    start_date = recent_date - pd.Timedelta(days=slider_nap)
                    sleep_filtered = sleep[(sleep["Date"] >= start_date) & (sleep["Date"] <= recent_date)].copy()
                    df_nap = sleep_filtered.dropna(subset=["Asleep_Nap"])[["Date", "Asleep_Nap"]].copy()
                    fig, ax = plt.subplots(figsize=(10, 4))
                    ax.bar(df_nap["Date"], df_nap["Asleep_Nap"], color="teal", width=0.8)
                    ax.set_title("Nap Asleep over time")
                    ax.set_ylabel("Minutes")
                    ax.tick_params(axis='x', rotation=45)
                    ax.xaxis.set_major_locator(mdates.DayLocator(interval=7))
                    ax.xaxis.set_major_formatter(mdates.DateFormatter('%b-%d'))
                    ax.grid(axis="y", alpha=0.25)
                    ax.set_axisbelow(True)
                    sns.despine(ax=ax)
                    st.pyplot(fig)

                    # Monthly total nap inbed
                    sleep_monthly = sleep_filtered.set_index("Date").resample("M")["Asleep_Nap"].sum().reset_index()
                    st.subheader("üóìÔ∏è Monthly Nap Asleep Total (min)")
                    plot_line(sleep_monthly.dropna(subset=["Asleep_Nap"]), "Date", "Asleep_Nap",
                            "Monthly Nap Asleep Total", "Minutes", 
                            marker="o", color="coral", xlabel="",
                            rotate_x=True, date_locator=mdates.MonthLocator(interval=1), show_grid=True, date_formatter=mdates.DateFormatter('%b-%Y'))
                else:
                    st.info("Column 'InBed_Nap' not found in sleep data.")

 
# =========================
# TAB 3 ‚Äî RECOVERY
# =========================
with tab3:
    st.header("üß† Recovery")

    if recovery is None:
        st.info("Upload your clean recovery CSV to see charts.")
    else:
        if "Date" not in recovery.columns:
            st.error("Recovery CSV must include a Date column.")
        else:
            recovery = recovery.sort_values("Date")

            st.subheader("üìã Recovery table")
            st.dataframe(recovery)

            # Main recovery score (sigmoid)
            if "Sigmoid Recovery Score" in recovery.columns:
                st.subheader("üß† Sigmoid Recovery Score (0‚Äì1)")
                plot_line(
                    recovery.dropna(subset=["Sigmoid Recovery Score"]),
                    "Date",
                    "Sigmoid Recovery Score",
                    "Sigmoid Recovery Score over time",
                    "Score", xlabel="", color="seagreen", rotate_x=True, 
                    date_locator=mdates.DayLocator(interval=2), 
                    date_formatter=mdates.DateFormatter('%b-%d')
                )

            # Components (choose what you want)
            st.subheader("üß© Components")
            candidates = [
                "Stress_prev_day",
                "Overnight HRV",
                "Resting Heart Rate",
                "Score",
                "RECOVERY_SCORE_RAW",
            ]
            available = [c for c in candidates if c in recovery.columns]
            if available:
                chosen = st.multiselect("Pick component(s) to plot:", available, default=available[:3])
                for col in chosen:
                    plot_line(recovery.dropna(subset=[col]), "Date", col, f"{col} over time", col)
            else:
                st.info("No component columns detected (Stress_prev_day / Overnight HRV / etc.).")

# =========================
# TAB 4 ‚Äî CORRELATIONS
# =========================
with tab4:
    st.header("üîó Correlations (Workout vs Recovery/Sleep)")

    if workouts is None or (sleep is None and recovery is None):
        st.info("Upload workouts + (sleep and/or recovery) to compute correlations.")
    else:
        # Workout weekly aggregation (works even if you train 3‚Äì4x/week)
        if "VOLUME" in workouts.columns:
            tmp = workouts.copy()
            tmp["Week"] = weekly_bucket(tmp["Date"])
            wk_total = tmp.groupby("Week", as_index=False).agg(
                WeeklyVolume=("VOLUME", "sum"),
                WeeklyRPE=("RPE", "mean") if "RPE" in tmp.columns else ("VOLUME", "size")
            )

            st.subheader("üì¶ Weekly Training Load")
            st.dataframe(wk_total.sort_values("Week"))

            # Recovery weekly aggregation
            if recovery is not None and "Sigmoid Recovery Score" in recovery.columns:
                rec = recovery.copy()
                rec["Week"] = weekly_bucket(rec["Date"])
                wk_rec = rec.groupby("Week", as_index=False).agg(
                    WeeklyRecovery=("Sigmoid Recovery Score", "mean"),
                    WeeklyStressPrev=("Stress_prev_day", "mean") if "Stress_prev_day" in rec.columns else ("Sigmoid Recovery Score", "size")
                )

                merged = pd.merge(wk_total, wk_rec, on="Week", how="inner").sort_values("Week")
                st.subheader("üîó Weekly Volume vs Weekly Recovery")
                st.dataframe(merged)

                if not merged.empty:
                    corr = merged["WeeklyVolume"].corr(merged["WeeklyRecovery"])
                    st.write(f"**Correlation (WeeklyVolume vs WeeklyRecovery):** {corr:.2f} (n={len(merged)})")

                    fig, ax = plt.subplots(figsize=(6, 4))
                    ax.scatter(merged["WeeklyVolume"], merged["WeeklyRecovery"])
                    ax.set_xlabel("WeeklyVolume (lb¬∑reps)")
                    ax.set_ylabel("WeeklyRecovery (mean sigmoid)")
                    ax.set_title("Weekly Volume vs Weekly Recovery")
                    ax.grid(True, alpha=0.25)
                    st.pyplot(fig)

                    # Optional: show both over time
                    st.subheader("üìà Weekly Volume + Recovery (time)")
                    plot_two_axis(
                        merged, "Week",
                        "WeeklyVolume", "WeeklyRecovery",
                        "Weekly Volume vs Recovery over time",
                        "WeeklyVolume", "WeeklyRecovery"
                    )

            # Sleep weekly aggregation
            sleep_score_col = pick_col(sleep, ["Score", "Sleep Score", "SleepScore", "SCORE", "Score.1", "Score.2"]) if sleep is not None else None

            if sleep is not None and sleep_score_col is not None:
                sl = sleep.copy()
                sl["Week"] = weekly_bucket(sl["Date"])
                wk_sl = sl.groupby("Week", as_index=False).agg(
                    WeeklySleepScore=(sleep_score_col, "mean"),
                    WeeklyAsleep=("Asleep hrs", "mean") if "Asleep hrs" in sl.columns else (sleep_score_col, "size"),
                )

            else:
                st.info("Workouts file needs a VOLUME column to compute weekly load correlations.")
# =========================
# TAB 5 ‚Äî STATS
# =========================
with tab5:
    # Data agg necessary for hypothesis testing
    recovery["Date"] = pd.to_datetime(recovery["Date"])  # Convert to datetime
    workouts_daily = workouts.groupby("Date", as_index=False).agg({
        "DURATION_MIN": "max",
        "WEIGHT_LBS": "max",
        "VOLUME": "sum",
        "RPE": "mean",
        "est_1RM": "mean"})
    recovery_exercise = pd.merge(recovery, workouts_daily, on="Date", how="left").sort_values("Date")
    recovery_exercise["Exercise_Done"] = recovery_exercise["VOLUME"].fillna(0).gt(0).astype(int)
    recovery_exercise_done = recovery_exercise[recovery_exercise["Exercise_Done"] == 1].copy()
    recovery_exercise_notdone = recovery_exercise[recovery_exercise["Exercise_Done"] == 0].copy()
    #-----------------------------
    st.header("üìâ Stats")
    st.subheader("üìä Recovery on Exercise vs Non-Exercise Days")
    allowed = sorted(["InBed hrs", "Asleep hrs", "Wake Count", "REM hrs", "Light hrs", "Deep hrs", "Fall Asleep", 
            "Overnight HRV", "Stress", "RHR", "Score"])
    check_metric = st.selectbox("Select metric to analyze:", allowed, index=allowed.index("Score"))
    picked_col = recovery[check_metric] if recovery is not None and check_metric in recovery.columns else None
    mean_val = picked_col.mean()
    median_val = picked_col.median()
    std_val = picked_col.std()
    trim_mean_val = stats.trim_mean(picked_col.dropna(), 0.1) if picked_col is not None else None
    n = picked_col.dropna().shape[0] 
    trim_mean = stats.trim_mean(picked_col.dropna(), 0.1) if picked_col is not None else None
    cv = std_val / mean_val if mean_val not in (0, None) and not pd.isna(mean_val) else np.nan
    col_pvalue, col_inter = normality_test(picked_col) if picked_col is not None else (None, None)
    #------------------------------------- 4 MOMENTS OF DATA ----------------------------------------
    with st.expander("üéØ Four Moments of Statistics", expanded=False):
        st.subheader("üéØ Four Moments of Statistics")
        c1, c2, c3, c4, c5, c6, c7, c8 = st.columns(8)
        series_for_chart = picked_col.dropna().astype(float).tail(30).tolist() if picked_col is not None else []
        c1.metric("Median", f"{median_val:.2f}" if median_val is not None else "‚Äî")
        c2.metric(
            "Mean",
            f"{mean_val:.2f}" if mean_val is not None and not pd.isna(mean_val) else "‚Äî",
            chart_data=series_for_chart if len(series_for_chart) >= 2 else None,
            chart_type="line")
        c3.metric("Std Dev", f"{std_val:.2f}" if std_val is not None else "‚Äî")
        c4.metric("Trimmed Mean (10%)", f"{trim_mean:.2f}" if trim_mean is not None else "‚Äî")
        c5.metric("Sample (n)", "Sufficient" if n >= 30 else "Insufficient", 
                  delta=n, delta_color="normal" if n >= 30 else "inverse",
                  help="n >=30 is considered sufficient for Central Limit Theorem.", delta_arrow="off")
        c6.metric("Coef of Var (CV)", "Good" if cv is not None and cv < 0.1 else "Acceptable" if cv is not None and cv < 0.2 else "High", 
                  delta=f"{round(cv*100, 2)} %",delta_color="normal" if cv is not None and cv < 0.1 else "orange" if cv is not None and cv < 0.2 else "inverse" if cv is not None else None,
                  help="CV <10% is considered good stability; 10-20% acceptable; >20% high variability.")
        c7.metric("Skewness", f"{picked_col.skew():.2f}" if picked_col is not None else "‚Äî", help="Skewness indicates asymmetry. >0 means right-skewed, <0 means left-skewed.")
        c8.metric("Kurtosis", f"{picked_col.kurtosis():.2f}" if picked_col is not None else "‚Äî", help="Kurtosis indicates the 'tailedness' of the distribution. >3 means heavy tails, <3 means light tails.",
                  delta="Leptokurtic" if picked_col is not None and picked_col.kurtosis() > 3 else "Platykurtic" if picked_col is not None and picked_col.kurtosis() < 3 else "Mesokurtic" if picked_col is not None else None,
                  delta_arrow="off", delta_color="green" if picked_col is not None and picked_col.kurtosis() == 3 else "red" if picked_col is not None and picked_col.kurtosis() > 3 else "green" if picked_col is not None and picked_col.kurtosis() < 3 else None)
    #------------------------------------- EMPIRICAL CDF & PERCENTILES ----------------------------------------
    with st.expander("üìä Empirical CDF & Percentiles", expanded=False):
        st.subheader("üìä Empirical CDF & Percentiles")
        c1, c2 = st.columns(2)
        with c1:
            if picked_col is not None and not picked_col.dropna().empty:
                complementary = st.segmented_control("Complementary CDF ?:", [True, False], key="cdf_type_control", default=True, 
                                                     help="If True then complementary CDF (1 - CDF) is shown.")
                perc_90 = picked_col.quantile(0.9)
                perc_75 = picked_col.quantile(0.75)
                fig, ax = plt.subplots(figsize=(10, 4))
                cecdf_50 = compute_ecdf(picked_col.dropna(), median_val, complementary=complementary)
                cecdf_75 = compute_ecdf(picked_col.dropna(), perc_75, complementary=complementary)
                cecdf_90 = compute_ecdf(picked_col.dropna(), perc_90, complementary=complementary)
                sns.ecdfplot(data=recovery, x=check_metric, label=f"Empirical CDF {check_metric}", 
                            color="green", complementary=complementary, ax=ax)
                plt.axvline(mean_val, color="blue", linestyle="--", label=f"Mean: {mean_val:.2f}", linewidth=0.5)
                plt.axvline(median_val, color="lightseagreen", linestyle=":", label=f"50th Percentile: {median_val:.2f}", linewidth=1)
                plt.axvline(perc_90, color="yellow", linestyle="--", label=f"90th Percentile: {perc_90:.2f}", linewidth=0.5)
                plt.axvline(perc_75, color="brown", linestyle="--", label=f"75th Percentile: {perc_75:.2f}", linewidth=0.5)
                if complementary:
                    plt.title(f"Complementary ECDF of {check_metric}", fontsize=14, fontweight="bold", pad=15)
                    plt.ylabel("Complementary ECDF")
                else:
                    plt.title(f"Empirical CDF of {check_metric}", fontsize=14, fontweight="bold", pad=15)
                    plt.ylabel("ECDF")
                plt.xlabel(check_metric)
                plt.legend(loc="best", fontsize=7)
                sns.despine()
                st.pyplot(fig)
            else:
                st.warning(f"No data available for {check_metric} to plot ECDF.")
        with c2:
            st.subheader(f"üìà Percentile Insights {check_metric}")
            st.write("Typical value (median)", round(median_val,2))
            st.write("Uncommon value (75th ):", round(perc_75,2))   
            st.write("Rare high value (90th):", round(perc_90,2))
            st.subheader(f"üìä Probability Insights {check_metric}")
            if complementary:
                st.write("The probability of exceeding", round(median_val,2), " is:" , round((cecdf_50)*100, 2), " %")
                st.write("The probability of exceeding", round(perc_75,2), " is:", round((cecdf_75)*100, 2), " %")
                st.write("The probability of exceeding", round(perc_90,2), " is:", round((cecdf_90)*100, 2), " %")
            else:
                st.write("The probability of getting any value up to my typical performance is:", round(cecdf_50*100, 2), " %")
                st.write("The probability of getting any value up to common performance is:", round(cecdf_75*100, 2), " %")
                st.write("The probability of getting any value up to atypical performance is:", round(cecdf_90*100, 2), " %")
            st.info("Note: Even though the ECDF provides empirical probabilities based on historical data, " \
            "it does not guarantee future outcomes. Use this information as a guide rather than a definitive prediction.", 
            icon="‚ÑπÔ∏è")
    #------------------------------------- NORMALITY TEST & VISUALS ----------------------------------------
    with st.expander("üîç Normality Test for Recovery", expanded=False):
        st.subheader("üîç Normality Test for Recovery")
        
        # Interpretation
        if col_pvalue is not None and col_pvalue > 0.05:
            st.success(f"Shapiro Wilk Test: {check_metric} appears to be normally distributed (p={col_pvalue:.3f}). You can use parametric tests.")
        elif col_pvalue is not None and col_pvalue <= 0.05:
            st.info(f"Shapiro Wilk Test: {check_metric} does not appear to be normally distributed (p={col_pvalue:.3f}). You may want to use non-parametric tests.")
        else:
            st.info(f"Not enough data to perform normality test on {check_metric}.")
        try:
            distributions = pd.to_numeric(picked_col, errors="coerce").dropna()
            st.dataframe(fit_distribution(distributions))
        except Exception as e:
            st.error(f"Error fitting distribution: {e}")
        bins = st.slider("Select number of bins for histogram", 5, 50, 20, 1, width=250, key="hist_bins_slider")
        c1, c2 = st.columns(2)
        with c1:
            #Plot histogram
            fig, ax = plt.subplots(figsize=(7,3))
            sns.histplot(picked_col.dropna(), kde=True, ax=ax, stat="probability", bins=bins)
            ax.axvline(mean_val, color="blue", linestyle="--", label=f"Mean: {mean_val:.2f}")
            ax.axvline(median_val, color="red", linestyle=":", label=f"Median: {median_val:.2f}")
            ax.axvline(trim_mean_val, color="green", linestyle="-.", label=f"Trimmed Mean: {trim_mean_val:.2f}")
            ax.axvspan(mean_val - std_val, mean_val + std_val, color="yellow", alpha=0.15, label=f"¬±1 Std Dev: {std_val:.2f}")
            ax.set_title(f"Histogram of {check_metric}", fontsize=14, fontweight="bold", pad=15)
            ax.legend(loc="best", fontsize=7)
            ax.set_xlabel(check_metric)
            ax.set_ylabel("Probability")
            sns.despine(ax=ax)
            st.pyplot(fig)
        with c2:
            fig, ax = plt.subplots(figsize=(7,3))
            sns.boxplot(y=picked_col.dropna(), ax=ax, width=0.3, fliersize=3, flierprops={"markerfacecolor": "red", "marker": "o"})
            sns.stripplot(y=picked_col.dropna(), ax=ax, color="lightblue", size=4, jitter=True, alpha=0.15)
            ax.set_title(f"Boxplot of {check_metric}", fontsize=14, fontweight="bold", pad=15)
            ax.set_xlabel(check_metric)
            sns.despine(ax=ax)
            st.pyplot(fig)

        if  st.checkbox("Show full time series plot", value=True, key="full_time_series_checkbox"):
            fig, ax = plt.subplots(figsize=(10, 3))
            sns.lineplot(data=recovery, x="Date", y=check_metric, ax=ax, linewidth=1)
            ax.axhline(mean_val, color="blue", linestyle="--", label=f"Mean: {mean_val:.2f}")
            ax.axhline(median_val, color="red", linestyle=":", label=f"Median: {median_val:.2f}")
            ax.axhline(trim_mean_val, color="green", linestyle="-.", label=f"Trimmed Mean: {trim_mean_val:.2f}")
            ax.axhspan(mean_val - std_val, mean_val + std_val, color="yellow", alpha=0.05, label=f"¬±1 Std Dev: {std_val:.2f}")
            ax.legend(loc="best", fontsize=5)
            ax.set_title(f"Time Series of {check_metric}", fontsize=14, fontweight="bold", pad=15)
            ax.set_xlabel("")
            ax.set_ylabel(check_metric)
            sns.despine(ax=ax)
            ax.tick_params(axis='x', rotation=45)
            st.pyplot(fig)
        else:
            date_plot = st.slider("Select number of days to show for time series plot", 30, 365, 180, 1, key="time_series_days_slider", 
                width=250)
            date_filter = recovery["Date"].max() - pd.Timedelta(days=date_plot)
            fig, ax = plt.subplots(figsize=(10, 3))
            sns.lineplot(data=recovery[recovery["Date"] >= date_filter], x="Date", y=check_metric, ax=ax, linewidth=1)
            ax.axhline(mean_val, color="blue", linestyle="--", label=f"Mean: {mean_val:.2f}")
            ax.axhline(median_val, color="red", linestyle=":", label=f"Median: {median_val:.2f}")
            ax.axhline(trim_mean_val, color="green", linestyle="-.", label=f"Trimmed Mean: {trim_mean_val:.2f}")
            ax.axhspan(mean_val - std_val, mean_val + std_val, color="yellow", alpha=0.05, label=f"¬±1 Std Dev: {std_val:.2f}")
            ax.legend(loc="best", fontsize=5)
            ax.set_title(f"Time Series of {check_metric}", fontsize=14, fontweight="bold", pad=15)
            ax.set_xlabel("")
            ax.set_ylabel(check_metric)
            sns.despine(ax=ax)
            ax.tick_params(axis='x', rotation=45)
            st.pyplot(fig)
    # ------------------------------------- OUTLIERS DETECTION ----------------------------------------
    with st.expander("üß™ Outliers Detection", expanded=False):
        st.subheader("üß™ Outliers Detection")
        outliers_iqr = outlier_dectection_iqr(picked_col) if picked_col is not None else pd.Series(dtype=float)
        outliers_z = outlier_detection_zscore_modified(picked_col, threshold=3)

        if len(outliers_iqr) == 0 and len(outliers_z) == 0: 
            st.success(f"No outliers detected in {check_metric} using IQR method and Modified Z-Score method.", icon="‚úÖ")
        else:
            st.info(f"Detected {len(outliers_iqr)} outlier(s) in {check_metric} using IQR method.",
                    icon="‚ÑπÔ∏è")
            st.dataframe(outliers_iqr.to_frame(name=f"{check_metric} Value"))
            st.info(f"Detected {len(outliers_z)} outlier(s) in {check_metric} using Modified Z-Score method.",
                    icon="üö®")
            st.dataframe(outliers_z.to_frame(name=f"{check_metric} Value"))
    
    #------------------------------------- HYPOTHESIS TESTING ----------------------------------------
    with st.expander("üõ†Ô∏è Tests with rest days and exercise days", expanded=False):
        st.subheader("üõ†Ô∏è Statistical Tests")
        if col_pvalue > 0.05:
            group1 = recovery_exercise_done[check_metric].dropna()
            group2 = recovery_exercise_notdone[check_metric].dropna() 
            st.write("Since the data appears to be normally distributed, you can use parametric tests such as t-tests or ANOVA for further analysis.")
            options = ["One-sample t-test", "Independent t-test"]
            choice = st.segmented_control(
                "Select test to perform:",
                options=options, key="parametric_tests_control")
            if choice == "One-sample t-test":
                c1, c2 = st.columns(2)
                with c1:
                    option_df = st.selectbox("Select data group to test against population mean:",
                                             ["Exercise Days", "Rest Days", "All Days"])
                    if option_df == "Exercise Days":
                        group = recovery_exercise_done[check_metric].dropna()
                        st.warning("T Test are about means. They test whether the mean is different from the population mean (One-sample test) or whether the means of the groups are significantly different (Independent test).", icon="‚ö†Ô∏è")
                        popmean = st.number_input("Enter population mean to compare against:", value=float(mean_val) if mean_val is not None and not pd.isna(mean_val) else 0.0)
                        alternative = st.selectbox("Select alternative hypothesis:", ["two-sided", "less", "greater"])
                        ttest_res = stats.ttest_1samp(group, popmean, alternative=alternative)
                        button_run = st.button("Run One-sample t-test")
                        if button_run:
                            st.write(f"t-statistic: {ttest_res.statistic:.3f}," f" p-value: {ttest_res.pvalue:.3f}")
                            if ttest_res.pvalue < 0.05:
                                st.success("Reject the null hypothesis at Œ±=0.05 level.")
                            else:
                                st.info("Fail to reject the null hypothesis at Œ±=0.05 level.")
                    elif option_df == "Rest Days":
                        group = recovery_exercise_notdone[check_metric].dropna()
                        st.warning("T Test are about means. They test whether the mean is different from the population mean (One-sample test) or whether the means of the groups are significantly different (Independent test).", icon="‚ö†Ô∏è")
                        popmean = st.number_input("Enter population mean to compare against:", value=float(mean_val) if mean_val is not None and not pd.isna(mean_val) else 0.0)
                        alternative = st.selectbox("Select alternative hypothesis:", ["two-sided", "less", "greater"])
                        ttest_res = stats.ttest_1samp(group, popmean, alternative=alternative)
                        button_run = st.button("Run One-sample t-test")
                        if button_run:
                            st.write(f"t-statistic: {ttest_res.statistic:.3f}," f" p-value: {ttest_res.pvalue:.3f}")
                            if ttest_res.pvalue < 0.05:
                                st.success("Reject the null hypothesis at Œ±=0.05 level.")
                            else:
                                st.info("Fail to reject the null hypothesis at Œ±=0.05 level.")
                    elif option_df == "All Days":
                        group = recovery[check_metric].dropna()
                        st.warning("T Test are about means. They test whether the mean is different from the population mean (One-sample test) or whether the means of the groups are significantly different (Independent test).", icon="‚ö†Ô∏è")
                        popmean = st.number_input("Enter population mean to compare against:", value=float(mean_val) if mean_val is not None and not pd.isna(mean_val) else 0.0)
                        alternative = st.selectbox("Select alternative hypothesis:", ["two-sided", "less", "greater"])
                        ttest_res = stats.ttest_1samp(group, popmean, alternative=alternative)
                        button_run = st.button("Run One-sample t-test", key="one_sample_ttest_button")
                        if button_run:
                            st.write(f"t-statistic: {ttest_res.statistic:.3f}," f" p-value: {ttest_res.pvalue:.3f}")
                            if ttest_res.pvalue < 0.05:
                                st.success("Reject the null hypothesis at Œ±=0.05 level.")
                            else:
                                st.info("Fail to reject the null hypothesis at Œ±=0.05 level.")
                with c2:
                    fig, ax = plt.subplots(figsize=(7,5))
                    sns.kdeplot(group, color="lightblue", label=f"{option_df}", ax=ax)
                    ax.axvline(group.mean(), color="blue", linestyle="--", label=f"Exercise Mean: {group.mean():.2f}")
                    ax.axvline(popmean, color="red", linestyle="--", label=f"Population Mean: {popmean:.2f}")
                    sns.despine(ax=ax)
                    plt.title(f"Distribution of {check_metric}")
                    plt.xlabel(check_metric)
                    plt.ylabel("Density")
                    plt.legend(loc="best", fontsize=7)
                    st.pyplot(fig)

            elif choice == "Independent t-test":
                c1, c2 = st.columns(2)
                with c1:
                    st.warning("T Test are about means. They test whether the mean is different from the population mean (One-sample test) or whether the means of the groups are significantly different (Independent test).", icon="‚ö†Ô∏è")
                    st.info(f"Group 1 is {check_metric} with exercise ({group1.shape[0]} samples)")
                    st.info(f"Group 2 is {check_metric} on rest days {group2.shape[0]} samples)")
                    st.write("Exercise mean:", float(group1.mean()))
                    st.write("Rest mean:", float(group2.mean()))
                    st.write("Œî mean (ex - rest):", float(group1.mean() - group2.mean()))
                    alternative = st.selectbox("Select alternative hypothesis:", ["two-sided", "less", "greater"])
                    ttest2_res = stats.ttest_ind(group1, group2, alternative=alternative, equal_var=False)
                    button_run2 = st.button("Run Independent t-test", key="independent_ttest_button")
                    if button_run2:
                        st.write(f"t-statistic: {ttest2_res.statistic:.3f}," f" p-value: {ttest2_res.pvalue:.3f}")
                        if ttest2_res.pvalue < 0.05:
                            st.success("Reject the null hypothesis at Œ±=0.05 level.")
                        else:
                            st.info("Fail to reject the null hypothesis at Œ±=0.05 level.")
                with c2:
                    fig, ax = plt.subplots(figsize=(7,5))
                    sns.kdeplot(group1, color="lightblue", label="Exercise Days", ax=ax)
                    sns.kdeplot(group2, color="salmon", label="Rest Days", ax=ax)
                    ax.axvline(group1.mean(), color="blue", linestyle="--", label=f"Exercise Mean: {group1.mean():.2f}")
                    ax.axvline(group2.mean(), color="red", linestyle="--", label=f"Rest Mean: {group2.mean():.2f}")
                    sns.despine(ax=ax)
                    plt.title(f"Distribution of {check_metric}")
                    plt.xlabel(check_metric)
                    plt.ylabel("Density")
                    plt.legend(loc="best", fontsize=7)
                    st.pyplot(fig)
        elif col_pvalue <= 0.05:
            st.write("Since the data does not appear to be normally distributed, you can use non-parametric tests such as the Wilcoxon signed-rank test or the Mann-Whitney U test for further analysis.")
            options = ["Spearman Correlation", "Mann-Whitney U test"]
            choice = st.segmented_control(
                "Select test to perform:",
                options=options)
            if choice == "Spearman Correlation":
                col2 = st.selectbox("Select another metric to correlate with:", allowed, index=allowed.index("Stress"))
                # Align pairs by dropping rows where either metric is NaN
                df_pair = recovery[[check_metric, col2]].dropna()
                st.info(f"Using {df_pair.shape[0]} paired observations for correlation.")
                npairs = df_pair.shape[0]
                if npairs < 500:
                    st.warning("Spearman correlation is accurate for large samples (over 500 samples). For smaller samples, interpret results with caution.", icon="‚ö†Ô∏è")
                if df_pair.shape[0] < 2:
                    st.warning("Need at least 2 paired observations to compute correlation.")
                else:
                    x = df_pair[check_metric].astype(float)
                    y = df_pair[col2].astype(float)
                    try:
                        alternative = st.selectbox("Select alternative hypothesis for Spearman correlation:", ["two-sided", "less", "greater"], key="spearman_alternative_selectbox")
                        spearman_corr = stats.spearmanr(x, y, alternative=alternative)
                    except Exception as e:
                        st.error(f"Could not compute Spearman correlation: {e}")
                    else:
                        button_run = st.button("Run Spearman Correlation", key="spearman_corr_button")
                        if button_run:
                            c1, c2 = st.columns(2)
                            with c1:
                                coef_raw = spearman_corr.statistic
                                p_raw = spearman_corr.pvalue

                                coef_spearman = coef_raw[0, 1] if hasattr(coef_raw, "ndim") and coef_raw.ndim > 0 else coef_raw
                                p_value_val = p_raw[0, 1] if hasattr(p_raw, "ndim") and p_raw.ndim > 0 else p_raw

                                st.write(f"Spearman correlation coefficient: {coef_spearman}")
                                st.write(f"p-value: {p_value_val}")

                                if p_value_val < 0.05:
                                    st.success("Reject the null hypothesis at Œ±=0.05 level.")
                                else:
                                    st.info("Fail to reject the null hypothesis at Œ±=0.05 level.")   
                            with c2:
                                fig, ax = plt.subplots(figsize=(7,5))
                                sns.scatterplot(x=x, y=y, ax=ax, alpha=0.7)
                                sns.regplot(x=x, y=y, lowess=True, scatter=False, ax=ax, color="orange")
                                ax.set_title(f"Spearman correlation (Spearman coef = {coef_spearman:.2f}, p = {p_value_val:.3f})", fontsize=14, fontweight="bold", pad=15)
                                ax.set_xlabel(check_metric)
                                ax.set_ylabel(col2)
                                sns.despine(ax=ax)
                                st.pyplot(fig)            

            elif choice == "Mann-Whitney U test":
                group1 = recovery_exercise_done[check_metric].dropna()
                group2 = recovery_exercise_notdone[check_metric].dropna()
                c1, c2 = st.columns(2)
                with c1:
                    st.warning("Mann-Whitney U is about distributions, not means. It tests whether values from one group tend to be higher than the other.", icon="‚ö†Ô∏è")
                    st.info(f"Group 1 is {check_metric} with exercise ({group1.shape[0]} samples)")
                    st.info(f"Group 2 is {check_metric} on rest days ({group2.shape[0]} samples)")
                    st.write("Exercise median:", float(group1.median()))
                    st.write("Rest median:", float(group2.median()))
                    st.write("Œî median (ex - rest):", float(group1.median() - group2.median()))
                    alternative = st.selectbox("Select alternative hypothesis:", ["two-sided", "less", "greater"])
                    button_run2 = st.button("Run Mann-Whitney U test", key="mwu_test_button")
                    if button_run2:
                        stats_mwu, pvalue_mwu = stats.mannwhitneyu(group1, group2, alternative=alternative)
                        st.write(f"U statistic: ", stats_mwu)
                        st.write(f"p-value: ", pvalue_mwu)
                        if pvalue_mwu < 0.05:
                            st.success("Reject the null hypothesis at Œ±=0.05 level.")
                        else:
                            st.info("Fail to reject the null hypothesis at Œ±=0.05 level.")
                        u = stats_mwu
                        n1 = len(group1)
                        n2 = len(group2)
                        # Calculate effect size
                        CLES = u / (n1 * n2)
                        st.write(f"Common Language Effect Size (CLES):", round(CLES, 2))
                        if CLES > 0.5:
                            st.write(f"{CLES*100:.1f}% chance that a randomly selected exercise day has a higher "
                                f"{check_metric} than a randomly selected rest day.")
                        elif CLES < 0.5:
                            st.write(f"{(1-CLES)*100:.1f}% chance that a randomly selected rest day has a higher "
                                f"{check_metric} than a randomly selected exercise day.")
                        else:
                            st.write("No difference between exercise and rest days in " + check_metric + ".")
                with c2:
                    fig, ax = plt.subplots(figsize=(7,5))
                    sns.kdeplot(group1, color="lightblue", label="Exercise Days", ax=ax)
                    sns.kdeplot(group2, color="salmon", label="Rest Days", ax=ax)
                    sns.despine(ax=ax)
                    plt.title(f"Distribution of {check_metric}", fontsize=14, fontweight="bold", pad=15)
                    plt.xlabel(check_metric)
                    plt.ylabel("Density")
                    plt.legend(loc="best")
                    st.pyplot(fig)
# =========================
# TAB 6 ‚Äî MODELS
# =========================
with tab6:
    st.header("‚öôÔ∏è Models")
    recovery = st.session_state.df_recovery.copy()
    recovery["Date"] = pd.to_datetime(recovery["Date"], errors="coerce")  # Convert to datetime
    recovery["Sleep_need_hrs"] = recovery["Sleep Need"].apply(string_to_decimal_hours)
    recovery["Efficiency"] = recovery["Efficiency"].str.replace('%', '').astype(float) 
    recovery["Sleep_hr_surplus"] =  recovery["Asleep hrs"] - recovery["Sleep_need_hrs"]

    predictors = ["REM hrs", "Stress_prev_day", "Deep hrs", "Wake Count", "Sleep_hr_surplus", "Respiration"]
    df_model = recovery[["Date"] + predictors + ["Score", "Quality"]].dropna().copy()
    df_model = df_model.sort_values("Date")

    st.write("Modeling on: ", df_model.shape[0], "samples with no missing values in selected features and Score.")
    model_version = "1.0.0"
    st.progress(text=f"Model Version: {model_version}", value=50)

    for col in predictors:
        if col not in df_model.columns:
            st.error(f"Predictor column '{col}' not found in data.")
            st.stop()
    models = st.segmented_control(
        "Select Model Type:",
        ["Linear Regression", "Logistic Regression"], key="model_type_control", default="Linear Regression")
    if models == "Linear Regression":    #Linear Regression Selected
            #------------------------------FROZEN MODEL CONDITIONALS-----------------------------
        if "model_frozen" not in st.session_state:
            st.session_state.model_frozen = None
        if "freeze_date" not in st.session_state:
            st.session_state.freeze_date = None
        if "freeze_predictors" not in st.session_state:
            st.session_state.freeze_predictors = None
        n = df_model.shape[0]
        if st.session_state.model_frozen is not None:
            if st.button("Reset frozen model (session)"):
                st.session_state.model_frozen = None
                st.session_state.freeze_date = None
                st.session_state.freeze_predictors = None
                st.rerun()
        #------------------------------OLS LINEAR REGRESSION TRAINING PHASE-----------------------------
        if (st.session_state.model_frozen is None) and (n < 150):
            st.warning("MODEL ON TRAINING PHASE YET",icon="spinner")
            with st.expander("üìê OLS Linear Regression: Insights", expanded=False):
                st.write("Used for prediction:", [f for f in predictors if f in df_model.columns])
                H=30    #Test size of 30 samples
                train_lin = df_model.iloc[:-H].copy()
                test_lin  = df_model.iloc[-H:].copy()
                #train_lin, test_lin = train_test_split(df_model, test_size=0.2, shuffle=False, stratify=None)
                X = sm.add_constant(train_lin[predictors])
                y = train_lin["Score"]
                model_linear = sm.OLS(y, X).fit(cov_type='HC3')

                X_test = sm.add_constant(test_lin[predictors])
                y_test = test_lin["Score"]
                y_pred_linear = model_linear.predict(X_test)

                r2_train_linear = model_linear.rsquared
                r2_test_linear = r2_score(y_test, y_pred_linear)
                mse_train_linear = mean_squared_error(y, model_linear.fittedvalues)
                mse_test_linear = mean_squared_error(y_test, y_pred_linear)
                mae_train_linear = mean_absolute_error(y, model_linear.fittedvalues)
                mae_test_linear = mean_absolute_error(y_test, y_pred_linear)
                rmse_train_linear = np.sqrt(mse_train_linear)
                rmse_test_linear = np.sqrt(mse_test_linear)
                # ----------------------------- PERFORMANCE METRICS -----------------------------
                st.subheader("üìà Train Set Performance")
                c1, c2, c3, c4, c5, c6 = st.columns(6)
                with c1:
                    st.metric("Train R¬≤", f"{r2_train_linear:.3f}")
                with c2:
                    st.metric("MSE", f"{mse_train_linear:.3f}")
                with c3:
                    st.metric("MAE", f"{mae_train_linear:.3f}")
                with c4:
                    st.metric("RMSE", f"{rmse_train_linear:.3f}")
                with c5:
                    st.metric("Samples", f"{train_lin.shape[0]}")
                with c6:
                    st.metric("Training Start Date", f"{train_lin.Date.min().date()}")

                st.subheader("üìâ Test Set Performance")
                c1, c2, c3, c4, c5, c6 = st.columns(6)
                with c1:
                    st.metric("Test R¬≤", f"{r2_test_linear:.3f}", delta=f"{r2_test_linear - r2_train_linear:.3f}", 
                            delta_color="green" if r2_test_linear > r2_train_linear else "red")
                with c2:
                    st.metric("MSE", f"{mse_test_linear:.3f}", delta=f"{mse_test_linear - mse_train_linear:.3f}", 
                            delta_color="red" if mse_test_linear > mse_train_linear else "green",
                            help="Mean Squared Error (MSE): lower values indicate better fit.\
                                 Penalizes larger errors more heavily.")
                with c3:
                    st.metric("MAE", f"{mae_test_linear:.3f}", delta=f"{mae_test_linear - mae_train_linear:.3f}", 
                            delta_color="red" if mae_test_linear > mae_train_linear else "green",
                            help="Mean Absolute Error (MAE): lower values indicate better fit.")
                with c4:
                    st.metric("RMSE", f"{rmse_test_linear:.3f}", delta=f"{rmse_test_linear - rmse_train_linear:.3f}", 
                            delta_color="red" if rmse_test_linear > rmse_train_linear else "green", 
                            help="Root Mean Squared Error (RMSE): lower values indicate better fit, in original units.")
                with c5:
                    st.metric("Samples", f"{test_lin.shape[0]}", help="The last 30 samples used for testing.")
                with c6:
                    st.metric("Test Start Date", f"{test_lin.Date.min().date()}")
                # ----------------------------- LEARNING CURVE ------------------------------------
                with st.expander("üìàüß† Learning Curve", expanded=False):
                    c1, c2 = st.columns(2)
                    with c1:

                        st.subheader("üìä Learning Curve Metrics at Key Sample Sizes")
                        sample_sizes = range(20, 160, 10)
                        results = []

                        for sample in sample_sizes:
                            res = metrics_learning_curve(df_model, sample, predictors)
                            if res is not None:
                                results.append(res)
                        if results:
                            df_all_metrics = pd.DataFrame(results).set_index("Model_samples")
                            st.dataframe(df_all_metrics)
                        else:
                            st.warning("Not enough data to compute learning curve metrics.") 

                        learning_curve_df = pd.DataFrame(results)

                        st.subheader("ü™úüß± Plateaut Detection")
                        learning_curve_df = learning_curve_df.set_index("Model_samples")
                        learning_curve_df["% Œî RMSE"] = (learning_curve_df["Test RMSE"].diff().fillna(0)*100) / learning_curve_df["Test RMSE"].shift(1).replace(0, np.nan)
                        st.dataframe(learning_curve_df[["Test RMSE","% Œî RMSE"]])
                        st.write("k=", 3)
                        st.write("% Œî no greater than", 5)

                        #def plateau_detection(df, k=3, threshold=5):
                        st.button("Deploy ?")
                    #----------------------------- PLOTTING LEARNING CURVE -----------------------------
                    with c2:
                        st.subheader("üìà Learning Curve Plot")
                        metric = st.selectbox("Select metric to plot:", ["MAE", "MSE", "RMSE"], index=2, key="learning_curve_metric_selectbox")
                        samples = st.checkbox("Show all values for learning curve (not forecast or extrapolated) ?:", value=True, key="learning_curve_future_values_checkbox")
                        fig, ax = plt.subplots(figsize=(10,5))
                        if samples:
                            sns.lineplot(data=learning_curve_df, x=learning_curve_df.index, y=f"Train {metric}",  label=f"Train {metric}", ax=ax, color="lightblue", linestyle=":", linewidth=1, marker="o", markersize=4)                        
                            sns.lineplot(data=learning_curve_df, x=learning_curve_df.index, y=f"Test {metric}",  label=f"Test {metric}", ax=ax, color="orange", linewidth=1, marker="x", markersize=4)
                            ax.axvspan(xmin=40, xmax=n, color="lightgrey", alpha=0.2, label="Current Region")
                        else:
                            filtered_lc = learning_curve_df.loc[learning_curve_df.index <= n]
                            sns.lineplot(data=filtered_lc, x=filtered_lc.index, y=f"Train {metric}", label=f"Train {metric}", ax=ax, color="lightblue", linestyle=":", linewidth=1, marker="o", markersize=2)                
                            sns.lineplot(data=filtered_lc, x=filtered_lc.index, y=f"Test {metric}", label=f"Test {metric}", ax=ax, color="orange", linewidth=1, marker="x")


                        ax.axvline(x=n, color="white", linestyle="--", label="Current Sample Size")
                        ax.set_title(f"Learning Curve: {metric} vs Training Size", fontweight="bold", fontsize=14, pad=15)
                        ax.set_xlabel("Model Samples")
                        ax.set_ylabel(metric)
                        ax.tick_params(axis='x', rotation=45)
                        ax.legend(loc="best", fontsize=7)
                        sns.despine(ax=ax)
                        st.pyplot(fig)

                        for params in model_linear.params.index:
                            if model_linear.pvalues[params] < 0.05:
                                st.success(f"{params} Coeff: {model_linear.params[params]:.4f} P-value: {model_linear.pvalues[params]:.4f} (Significant at Œ±=0.05)" )
                            else:
                                st.warning(f"{params} Coeff: {model_linear.params[params]:.4f} P-value: {model_linear.pvalues[params]:.4f}" )
     
                # ----------------------------- PREDICTIONS DATAFRAME -----------------------------
                df_model["Predicted_Score_Linear"] = model_linear.predict(sm.add_constant(df_model[predictors]))
                df_model["Predicted_Score_Linear_Test_Data"] = np.nan
                df_model.loc[test_lin.index, "Predicted_Score_Linear_Test_Data"] = model_linear.predict(
                    sm.add_constant(test_lin[predictors], has_constant="add"))
                df_model["Residuals_Linear"] = df_model["Score"] - df_model["Predicted_Score_Linear"]
                with st.expander("üóÇÔ∏è Predictions Dataframe", expanded=False):
                    st.dataframe(df_model[["Date", "Score", "Predicted_Score_Linear", "Predicted_Score_Linear_Test_Data", "Residuals_Linear"]].sort_values("Date"))
                # ----------------------------- MODEL SUMMARY & VISUALIZATIONS -----------------------------
            with st.expander("‚ÑπÔ∏è Model Summary", expanded=False):
                c1, c2 = st.columns(2)
                filtered_test = df_model.loc[test_lin.index].dropna(subset=["Predicted_Score_Linear_Test_Data"])
                test_start = filtered_test["Date"].min()
                test_end = filtered_test["Date"].max()
                with c1:
                    st.text(model_linear.summary().as_text())
                    st.subheader("üîó Correlations")
                    corr = df_model[predictors + ["Score"]].corr()
                    st.dataframe(corr)

                    for col in corr.columns:
                        if col != "Score":
                            correlation_insight(df_model, "Score", col)


                with c2:
                    fig, ax = plt.subplots(figsize=(10,5))
                    sns.lineplot(data=df_model, x="Date", y="Score", label="Actual", ax=ax, color="lightgreen", alpha=0.7)
                    sns.lineplot(data=df_model, x="Date", y="Predicted_Score_Linear", label="Predicted", ax=ax, linewidth=1, color="blue")
                    sns.lineplot(data=df_model, x="Date", y="Predicted_Score_Linear_Test_Data", label="Predicted (Test Data)", ax=ax, linestyle="--", linewidth=1, color="orange")
                    ax.axvspan(test_start, test_end, color="lightgrey", alpha=0.2, label="Test Set Period")
                    ax.set_title("Actual vs Predicted Sleep Score (Train & Test Set)", fontweight="bold", fontsize=14, pad=15)
                    ax.set_xlabel("")
                    ax.set_ylabel("Score")
                    ax.tick_params(axis='x', rotation=45)
                    ax.legend(loc="best", fontsize=7)
                    sns.despine(ax=ax)
                    st.pyplot(fig)

                    fig, ax = plt.subplots(figsize=(10,5))
                    sns.lineplot(data=filtered_test, x="Date", y="Score", label="Actual", ax=ax, marker="o", color="green", alpha=0.7)
                    sns.lineplot(data=filtered_test, x="Date", y="Predicted_Score_Linear_Test_Data", label="Predicted (Test Data)",
                                  ax=ax, marker="x", linewidth=1.5, linestyle=":", color="orange")
                    ax.set_xlabel("")
                    ax.set_ylabel("Score")
                    ax.set_title("Actual vs Predicted Sleep Score (Test Set)", fontweight="bold", fontsize=14, pad=15)
                    ax.tick_params(axis='x', rotation=45)
                    sns.despine(ax=ax)
                    ax.legend(loc="lower left", fontsize=7)
                    st.pyplot(fig)
                    st.markdown(f"**Out-of-sample Test R¬≤ (trained on train set):** {r2_test_linear:.3f}")

                    fig, ax = plt.subplots(figsize=(10,5))
                    sns.scatterplot(data=filtered_test, x="Score", y="Predicted_Score_Linear_Test_Data", ax=ax, color="purple", alpha=0.7, 
                                    hue="Quality", palette="viridis", legend="full")
                    sns.lineplot(data=filtered_test, x="Score", y="Score", ax=ax, color="red", linestyle="--", label="Ideal Fit")
                    ax.axvline(x=80, color="grey", linestyle=":", label="Good Quality Threshold")
                    ax.axvline(x=90, color="grey", linestyle=":", label="Excellent Quality Threshold")
                    ax.set_title("Predicted vs Actual Sleep Score (Test Set)", fontweight="bold", fontsize=14, pad=15)
                    ax.set_xlabel("Actual Sleep Score")
                    ax.set_ylabel("Predicted Sleep Score")
                    sns.despine(ax=ax)
                    ax.legend(loc="lower right", fontsize=7)
                    st.pyplot(fig)

            #----------------------------- RESIDUALS ANALYSIS -----------------------------
            with st.expander("üìä Residuals Analysis", expanded=False):
                option_residuals = ["On Train Set", "On Test Set", "Both"]
                residuals_analysis = st.segmented_control("Select residuals analysis dataset:", options=option_residuals, key="residuals_analysis")
                if residuals_analysis == "On Train Set":
                    st.subheader("üßæ Residuals Analysis on Train Set")
                    residuals_train_df = df_model[df_model.index.isin(train_lin.index)].copy()
                    residuals_train_df["Residuals"] = model_linear.resid
                    fitted = model_linear.fittedvalues
                    pvalue_resid, interpretation_resid = normality_test(residuals_train_df["Residuals"])
                    if pvalue_resid > 0.05:
                        st.success(f"Residuals appear to be normally distributed (p={pvalue_resid:.3f}).")
                    else:
                        st.info(f"Residuals do not appear to be normally distributed (p={pvalue_resid:.3f}).")
                    st.dataframe(fit_distribution(residuals_train_df["Residuals"]))
                    c1, c2 = st.columns(2)
                    with c1:
                        fig, ax = plt.subplots(figsize=(8,4))
                        sm.qqplot(residuals_train_df["Residuals"], line='45', ax=ax, fit=True)
                        ax.set_title("QQ Plot of Residuals (Train Set)", fontsize=14, fontweight="bold", pad=15)
                        sns.despine(ax=ax)
                        st.pyplot(fig)
                    with c2:
                        fig, ax = plt.subplots(figsize=(8,4))
                        sns.histplot(residuals_train_df["Residuals"], kde=True, stat="density", ax=ax)
                        ax.set_title("Histogram of Residuals (Train Set)", fontsize=14, fontweight="bold", pad=15)
                        ax.set_xlabel("Residuals")
                        sns.despine(ax=ax)
                        st.pyplot(fig)
                    c3, c4 = st.columns(2)
                    with c3:
                        fig, ax = plt.subplots(figsize=(8,4))
                        sns.scatterplot(x=fitted, y=residuals_train_df["Residuals"], ax=ax)
                        ax.axhline(0, color="red", linestyle="--")
                        ax.set_title("Residuals vs Fitted Values (Train Set)", fontsize=14, fontweight="bold", pad=15)
                        ax.set_xlabel("Fitted Values")
                        ax.set_ylabel("Residuals")
                        sns.despine(ax=ax)
                        st.pyplot(fig)
                    with c4:
                        fig, ax = plt.subplots(figsize=(8,4))
                        sns.lineplot(data=residuals_train_df, x="Date", y="Residuals", ax=ax)
                        ax.axhline(0, color="red", linestyle="--")
                        ax.set_title("Residuals over Time (Train Set)", fontsize=14, fontweight="bold", pad=15)
                        ax.set_xlabel("")
                        ax.set_ylabel("Residuals")
                        ax.tick_params(axis='x', rotation=45)
                        sns.despine(ax=ax)
                        st.pyplot(fig)

                elif residuals_analysis == "On Test Set":
                    st.subheader("üßæ Residuals Analysis on Test Set")
                    residuals_test_df = df_model[df_model.index.isin(test_lin.index)].copy()
                    residuals_test_df["Residuals"] = residuals_test_df["Score"] - residuals_test_df["Predicted_Score_Linear_Test_Data"]
                    fitted_test = residuals_test_df["Predicted_Score_Linear_Test_Data"]
                    pvalue_resid_test, interpretation_resid_test = normality_test(residuals_test_df["Residuals"])
                    if pvalue_resid_test > 0.05:
                        st.success(f"Residuals appear to be normally distributed (p={pvalue_resid_test:.3f}).")
                    else:
                        st.info(f"Residuals do not appear to be normally distributed (p={pvalue_resid_test:.3f}).")

                    c1, c2= st.columns(2)
                    with c1:
                        fig, ax = plt.subplots(figsize=(10,5))
                        sm.qqplot(residuals_test_df["Residuals"], line='45', ax=ax, fit=True)
                        ax.set_title("QQ Plot of Residuals (Test Set)", fontsize=14, fontweight="bold", pad=15)
                        sns.despine(ax=ax)
                        st.pyplot(fig)
                    with c2:
                        fig, ax = plt.subplots(figsize=(10,5))
                        sns.histplot(residuals_test_df["Residuals"], kde=True, stat="density", ax=ax)
                        ax.set_title("Histogram of Residuals (Test Set)", fontsize=14, fontweight="bold", pad=15)
                        ax.set_xlabel("Residuals")
                        sns.despine(ax=ax)
                        st.pyplot(fig)
                    c3, c4 = st.columns(2)
                    with c3:
                        fig, ax = plt.subplots(figsize=(10,5))
                        sns.scatterplot(x=fitted_test, y=residuals_test_df["Residuals"], ax=ax)
                        ax.axhline(0, color="red", linestyle="--")
                        ax.set_title("Residuals vs Fitted Values (Test Set)", fontsize=14, fontweight="bold", pad=15)
                        ax.set_xlabel("Fitted Values")
                        ax.set_ylabel("Residuals")
                        sns.despine(ax=ax)
                        st.pyplot(fig)
                    with c4:
                        fig, ax = plt.subplots(figsize=(10,5))
                        sns.lineplot(data=residuals_test_df, x="Date", y="Residuals", ax=ax)
                        ax.axhline(0, color="red", linestyle="--")
                        ax.set_title("Residuals over Time (Test Set)", fontsize=14, fontweight="bold", pad=15)
                        ax.set_xlabel("")
                        ax.set_ylabel("Residuals")
                        ax.tick_params(axis='x', rotation=45)
                        sns.despine(ax=ax)
                        st.pyplot(fig)
                elif residuals_analysis == "Both":
                    st.subheader("üßæ Residuals Analysis on Train & Test Set")
                    df_model["Residuals"] = df_model["Score"] - df_model["Predicted_Score_Linear"]
                    pvalue_resid_both, interpretation_resid_both = normality_test(df_model["Residuals"])
                    if pvalue_resid_both > 0.05:
                        st.success(f"Residuals appear to be normally distributed (p={pvalue_resid_both:.3f}).")
                    else:
                        st.info(f"Residuals do not appear to be normally distributed (p={pvalue_resid_both:.3f}).")
                    
                    c1, c2 = st.columns(2)
                    with c1:
                        fig, ax = plt.subplots(figsize=(10,5))
                        sm.qqplot(df_model["Residuals"], line='45', ax=ax, fit=True)
                        ax.set_title("QQ Plot of Residuals (All Data)", fontsize=14, fontweight="bold", pad=15)
                        sns.despine(ax=ax)
                        st.pyplot(fig)
                    with c2:
                        fig, ax = plt.subplots(figsize=(10,5))
                        sns.histplot(df_model["Residuals"], kde=True, stat="density", ax=ax)
                        ax.set_title("Histogram of Residuals (All Data)", fontsize=14, fontweight="bold", pad=15)
                        ax.set_xlabel("Residuals")
                        sns.despine(ax=ax)
                        st.pyplot(fig)
                    c3, c4 = st.columns(2)
                    with c3:
                        fig, ax = plt.subplots(figsize=(10,5))
                        sns.scatterplot(x=df_model["Predicted_Score_Linear"], y=df_model["Residuals"], ax=ax)
                        ax.axhline(0, color="red", linestyle="--")
                        ax.set_title("Residuals vs Fitted Values (All Data)", fontsize=14, fontweight="bold", pad=15)
                        ax.set_xlabel("Fitted Values")
                        ax.set_ylabel("Residuals")
                        sns.despine(ax=ax)
                        st.pyplot(fig)
                    with c4:
                        fig, ax = plt.subplots(figsize=(10,5))
                        sns.lineplot(data=df_model, x="Date", y="Residuals", ax=ax)
                        ax.axhline(0, color="red", linestyle="--")
                        ax.set_title("Residuals over Time (All Data)", fontsize=14, fontweight="bold", pad=15)
                        ax.set_xlabel("")
                        ax.set_ylabel("Residuals")
                        ax.tick_params(axis='x', rotation=45)
                        sns.despine(ax=ax)
                        st.pyplot(fig)
            
        # ------------------------------FROZEN MODEL DEPLOYMENT PHASE-----------------------------
        elif (st.session_state.model_frozen is None) and (n >= 150):
            st.success("MODEL READY FOR DEPLOYMENT (FREEZING NOW)", icon="‚úÖ")
            freeze_date = df_model.iloc[149]["Date"]  # Freeze after first 150 samples (0-149)
            frozen_df = df_model[df_model["Date"] <= freeze_date].copy()

            X_frozen = sm.add_constant(frozen_df[predictors])
            y_frozen = frozen_df["Score"]

            model_frozen = sm.OLS(y_frozen, X_frozen).fit(cov_type='HC3')

            st.session_state.model_frozen = model_frozen
            st.session_state.freeze_date = freeze_date
            st.session_state.freeze_predictors = predictors.copy()

            st.info(f"Frozen on {freeze_date.date()} New data after this will be monitored, not used for training.", icon="‚ÑπÔ∏è")
        #------------------------------ DEPLOYMENT & MONITORING -----------------------------
        else:
            st.success("MODEL DEPLOYED (SESSION-FROZEN)", icon="‚úÖ")
            # Safety: predictors must match
            if st.session_state.freeze_predictors != predictors:
                st.error("Predictors changed after model freeze. Please refresh/reset the model.")
                st.stop()

            model_frozen = st.session_state.model_frozen
            freeze_date = st.session_state.freeze_date

            # Predict for all rows with the frozen model
            X_all = sm.add_constant(df_model[predictors], has_constant="add")
            df_model["yhat_frozen"] = model_frozen.predict(X_all)
            df_model["resid_frozen"] = df_model["Score"] - df_model["yhat_frozen"]

            # Split into in-sample (<= freeze) and live (> freeze)
            live_df = df_model[df_model["Date"] > freeze_date].copy()

            st.caption(f"Frozen on {freeze_date.date()} | Live samples: {len(live_df)}")

            # Show latest prediction
            last = df_model.sort_values("Date").iloc[-1]
            st.metric("Latest predicted score", f"{last['yhat_frozen']:.2f}")

            # Monitoring metrics on LIVE (post-freeze)
            if len(live_df) >= 5:
                mae_live = mean_absolute_error(live_df["Score"], live_df["yhat_frozen"])
                rmse_live = np.sqrt(mean_squared_error(live_df["Score"], live_df["yhat_frozen"]))
                bias_live = live_df["resid_frozen"].mean()

                c1, c2, c3 = st.columns(3)
                c1.metric("Live MAE", f"{mae_live:.2f}")
                c2.metric("Live RMSE", f"{rmse_live:.2f}")
                c3.metric("Live Bias", f"{bias_live:.2f}")
            else:
                st.info("Not enough post-freeze samples yet for stable monitoring.")

            # Plot live residual drift
            with st.expander("üìä Monitoring: Live residuals", expanded=False):
                if len(live_df) > 0:
                    fig, ax = plt.subplots(figsize=(10,4))
                    sns.lineplot(data=live_df, x="Date", y="resid_frozen", ax=ax)
                    ax.axhline(0, color="red", linestyle="--")
                    ax.set_title("Post-freeze residuals over time")
                    ax.tick_params(axis='x', rotation=45)
                    st.pyplot(fig)

    elif models == "Logistic Regression":   #Toggle Logistic Regression Selected
        st.info("Logistic Regression coming soon!")

st.caption(
    "Tip: If you only train 3‚Äì4 days/week, use weekly aggregation (Volume / mean Recovery / mean Sleep) "
    "to avoid the mismatch between daily sleep and training frequency."
)